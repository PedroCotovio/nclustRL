{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from nclustRL.trainer import Trainer\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-30 11:23:48,562\tWARNING ppo.py:143 -- `train_batch_size` (1024) cannot be achieved with your other settings (num_workers=3 num_envs_per_worker=1 rollout_fragment_length=341)! Auto-adjusting `rollout_fragment_length` to 341.\n",
      "2021-12-30 11:23:48,562\tINFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20333)\u001b[0m Using backend: pytorch\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20329)\u001b[0m Using backend: pytorch\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m Using backend: pytorch\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20333)\u001b[0m 2021-12-30 11:23:53,682\tINFO worker.py:852 -- Calling ray.init() again after it has already been called.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m 2021-12-30 11:23:53,797\tINFO worker.py:852 -- Calling ray.init() again after it has already been called.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20329)\u001b[0m 2021-12-30 11:23:53,951\tINFO worker.py:852 -- Calling ray.init() again after it has already been called.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20333)\u001b[0m 2021-12-30 11:23:57,412\tINFO rollout_worker.py:1705 -- Validating sub-env at vector index=0 ... (ok)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20333)\u001b[0m 2021-12-30 11:23:57,412\tDEBUG rollout_worker.py:1534 -- Creating policy for default_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20333)\u001b[0m 2021-12-30 11:23:57,415\tINFO catalog.py:410 -- Wrapping <class 'nclustRL.models.model.RlModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m 2021-12-30 11:23:57,535\tINFO rollout_worker.py:1705 -- Validating sub-env at vector index=0 ... (ok)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m 2021-12-30 11:23:57,535\tDEBUG rollout_worker.py:1534 -- Creating policy for default_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m 2021-12-30 11:23:57,538\tINFO catalog.py:410 -- Wrapping <class 'nclustRL.models.model.RlModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m 2021-12-30 11:23:57,540\tINFO torch_policy.py:186 -- TorchPolicy (worker=1) running on 0.3333 GPU(s).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m /home/pedro.cotovio/Repos/nclustRL/nclustRL/models/model.py:147: UserWarning: Forward is running with random obs, this should only be acceptable during policy inicialization.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m   warnings.warn('Forward is running with random obs, this should only be acceptable during policy inicialization.')\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20333)\u001b[0m 2021-12-30 11:23:57,530\tINFO torch_policy.py:186 -- TorchPolicy (worker=3) running on 0.3333 GPU(s).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20333)\u001b[0m /home/pedro.cotovio/Repos/nclustRL/nclustRL/models/model.py:147: UserWarning: Forward is running with random obs, this should only be acceptable during policy inicialization.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20333)\u001b[0m   warnings.warn('Forward is running with random obs, this should only be acceptable during policy inicialization.')\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20329)\u001b[0m 2021-12-30 11:23:57,883\tINFO rollout_worker.py:1705 -- Validating sub-env at vector index=0 ... (ok)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20329)\u001b[0m 2021-12-30 11:23:57,884\tDEBUG rollout_worker.py:1534 -- Creating policy for default_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20329)\u001b[0m 2021-12-30 11:23:57,894\tINFO catalog.py:410 -- Wrapping <class 'nclustRL.models.model.RlModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20329)\u001b[0m 2021-12-30 11:23:57,901\tINFO torch_policy.py:186 -- TorchPolicy (worker=2) running on 0.3333 GPU(s).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20329)\u001b[0m /home/pedro.cotovio/Repos/nclustRL/nclustRL/models/model.py:147: UserWarning: Forward is running with random obs, this should only be acceptable during policy inicialization.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20329)\u001b[0m   warnings.warn('Forward is running with random obs, this should only be acceptable during policy inicialization.')\n",
      "2021-12-30 11:24:03,079\tINFO worker_set.py:104 -- Inferred observation/action spaces from remote worker (local worker has no env): {'default_policy': (Dict(action_mask:Box([0. 0. 0. 0.], [1. 1. 1. 1.], (4,), float32), avail_actions:Box([0. 0. 0. 0.], [1. 1. 1. 1.], (4,), float32), state:Box([100  10], [100  10], (2,), int32)), Tuple(Discrete(4), Tuple(Box([0. 0. 0.], [1. 1. 1.], (3,), float32), Box([0. 0. 0.], [1. 1. 1.], (3,), float32), Box([0. 0. 0.], [1. 1. 1.], (3,), float32), Box([0. 0. 0.], [1. 1. 1.], (3,), float32)))), '__env__': (Dict(action_mask:Box([0. 0. 0. 0.], [1. 1. 1. 1.], (4,), float32), avail_actions:Box([0. 0. 0. 0.], [1. 1. 1. 1.], (4,), float32), state:Box([100  10], [100  10], (2,), int32)), Tuple(Discrete(4), Tuple(Box([0. 0. 0.], [1. 1. 1.], (3,), float32), Box([0. 0. 0.], [1. 1. 1.], (3,), float32), Box([0. 0. 0.], [1. 1. 1.], (3,), float32), Box([0. 0. 0.], [1. 1. 1.], (3,), float32))))}\n",
      "2021-12-30 11:24:03,080\tDEBUG rollout_worker.py:1534 -- Creating policy for default_policy\n",
      "2021-12-30 11:24:03,088\tINFO catalog.py:410 -- Wrapping <class 'nclustRL.models.model.RlModel'> as None\n",
      "2021-12-30 11:24:03,092\tINFO torch_policy.py:186 -- TorchPolicy (worker=local) running on 0.0001 GPU(s).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m 2021-12-30 11:24:03,063\tDEBUG rollout_worker.py:728 -- Created rollout worker with env <ray.rllib.env.base_env._VectorEnvToBaseEnv object at 0x7f8363f935e0> (<TransformObservation<OrderEnforcing<BiclusterEnv<BiclusterEnv-v0>>>>), policies {}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20333)\u001b[0m 2021-12-30 11:24:03,067\tDEBUG rollout_worker.py:728 -- Created rollout worker with env <ray.rllib.env.base_env._VectorEnvToBaseEnv object at 0x7f3ac01ba070> (<TransformObservation<OrderEnforcing<BiclusterEnv<BiclusterEnv-v0>>>>), policies {}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20329)\u001b[0m 2021-12-30 11:24:03,113\tDEBUG rollout_worker.py:728 -- Created rollout worker with env <ray.rllib.env.base_env._VectorEnvToBaseEnv object at 0x7fb0ead12d60> (<TransformObservation<OrderEnforcing<BiclusterEnv<BiclusterEnv-v0>>>>), policies {}\n",
      "/home/pedro.cotovio/Repos/nclustRL/nclustRL/models/model.py:147: UserWarning: Forward is running with random obs, this should only be acceptable during policy inicialization.\n",
      "  warnings.warn('Forward is running with random obs, this should only be acceptable during policy inicialization.')\n",
      "2021-12-30 11:24:15,678\tINFO rollout_worker.py:1555 -- Built policy map: {}\n",
      "2021-12-30 11:24:15,687\tINFO rollout_worker.py:1556 -- Built preprocessor map: {'default_policy': None}\n",
      "2021-12-30 11:24:15,687\tINFO rollout_worker.py:618 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f8284d40a90>}\n",
      "2021-12-30 11:24:15,688\tDEBUG rollout_worker.py:728 -- Created rollout worker with env None (None), policies {}\n",
      "2021-12-30 11:24:15,785\tINFO trainable.py:124 -- Trainable.setup took 27.224 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2021-12-30 11:24:15,832\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    trainer=PPOTrainer,\n",
    "    env='BiclusterEnv-v0',\n",
    "    save_dir='/home/pedro.cotovio/Repos/nclustRL/Exp/Exp1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m 2021-12-30 11:28:39,890\tINFO sampler.py:1050 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m                                   'info': {},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m                                   'obs': { 'action_mask': np.ndarray((4,), dtype=float32, min=0.0, max=1.0, mean=0.5),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m                                            'avail_actions': np.ndarray((4,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m                                            'state': Graph(num_nodes={'col': 10, 'row': 100},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m       num_edges={('row', 'elem', 'col'): 1000},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m       metagraph=[('row', 'col', 'elem')])},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m                                   'prev_action': np.ndarray((13,), dtype=float64, min=-0.001, max=0.001, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m                                   'prev_reward': -0.001,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m /home/pedro.cotovio/Repos/nclustRL/venv/lib/python3.8/site-packages/numpy/core/_methods.py:178: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m 2021-12-30 11:28:39,912\tINFO sampler.py:1076 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m { 'default_policy': ( ( np.ndarray((1,), dtype=int32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m                         ( np.ndarray((1, 3), dtype=float32, min=-0.001, max=0.001, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m                           np.ndarray((1, 3), dtype=float32, min=0.001, max=0.001, mean=0.001),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m                           np.ndarray((1, 3), dtype=float32, min=-0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m                           np.ndarray((1, 3), dtype=float32, min=-0.001, max=0.001, mean=-0.0))),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 28), dtype=float32, min=-3.3999999521443642e+38, max=0.002, mean=-inf),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.215, max=-0.215, mean=-0.215)})}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m 2021-12-30 11:28:39,931\tINFO sampler.py:624 -- Raw obs from env: { 0: { 'agent0': { 'action_mask': np.ndarray((4,), dtype=float32, min=0.0, max=1.0, mean=0.5),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m                    'avail_actions': np.ndarray((4,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m                    'state': Graph(num_nodes={'col': 10, 'row': 100},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m       num_edges={('row', 'elem', 'col'): 1000},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m       metagraph=[('row', 'col', 'elem')])}}}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m 2021-12-30 11:28:39,931\tINFO sampler.py:626 -- Info return from env: {0: {'agent0': {}}}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m 2021-12-30 11:28:39,931\tINFO sampler.py:855 -- Filtered obs: { 'action_mask': np.ndarray((4,), dtype=float32, min=0.0, max=1.0, mean=0.5),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m   'avail_actions': np.ndarray((4,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m   'state': Graph(num_nodes={'col': 10, 'row': 100},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m       num_edges={('row', 'elem', 'col'): 1000},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m       metagraph=[('row', 'col', 'elem')])}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m 2021-12-30 11:28:48,101\tINFO simple_list_collector.py:781 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((341, 28), dtype=float32, min=-3.3999999521443642e+38, max=0.002, mean=-inf),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m               'action_logp': np.ndarray((341,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m               'actions': np.ndarray((341, 13), dtype=float32, min=-0.001, max=0.001, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m               'advantages': np.ndarray((341,), dtype=float32, min=0.001, max=0.111, mean=0.083),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m               'agent_index': np.ndarray((341,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m               'dones': np.ndarray((341,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m               'eps_id': np.ndarray((341,), dtype=int64, min=1293850576.0, max=1293850576.0, mean=1293850576.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m               'infos': np.ndarray((341,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m               'new_obs': { 'action_mask': np.ndarray((341, 4), dtype=float32, min=0.0, max=1.0, mean=0.5),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m                            'avail_actions': np.ndarray((341, 4), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m                            'state': np.ndarray((341,), dtype=object, head=Graph(num_nodes={'col': 10, 'row': 100},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m       num_edges={('row', 'elem', 'col'): 1000},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m       metagraph=[('row', 'col', 'elem')]))},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m               'obs': { 'action_mask': np.ndarray((341, 4), dtype=float32, min=0.0, max=1.0, mean=0.5),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m                        'avail_actions': np.ndarray((341, 4), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m                        'state': np.ndarray((341,), dtype=object, head=Graph(num_nodes={'col': 10, 'row': 100},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m       num_edges={('row', 'elem', 'col'): 1000},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m       metagraph=[('row', 'col', 'elem')]))},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m               'rewards': np.ndarray((341,), dtype=float32, min=-0.001, max=-0.001, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m               'unroll_id': np.ndarray((341,), dtype=int64, min=5.0, max=5.0, mean=5.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m               'value_targets': np.ndarray((341,), dtype=float32, min=-0.214, max=-0.104, mean=-0.132),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m               'vf_preds': np.ndarray((341,), dtype=float32, min=-0.215, max=-0.215, mean=-0.215)}}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m 2021-12-30 11:28:48,108\tINFO rollout_worker.py:792 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m { 'action_dist_inputs': np.ndarray((341, 28), dtype=float32, min=-3.3999999521443642e+38, max=0.002, mean=-inf),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m   'action_logp': np.ndarray((341,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m   'actions': np.ndarray((341, 13), dtype=float32, min=-0.001, max=0.001, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m   'advantages': np.ndarray((341,), dtype=float32, min=0.001, max=0.111, mean=0.083),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m   'agent_index': np.ndarray((341,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m   'dones': np.ndarray((341,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m   'eps_id': np.ndarray((341,), dtype=int64, min=1293850576.0, max=1293850576.0, mean=1293850576.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m   'infos': np.ndarray((341,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m   'new_obs': { 'action_mask': np.ndarray((341, 4), dtype=float32, min=0.0, max=1.0, mean=0.5),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m                'avail_actions': np.ndarray((341, 4), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m                'state': np.ndarray((341,), dtype=object, head=Graph(num_nodes={'col': 10, 'row': 100},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m       num_edges={('row', 'elem', 'col'): 1000},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m       metagraph=[('row', 'col', 'elem')]))},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m   'obs': { 'action_mask': np.ndarray((341, 4), dtype=float32, min=0.0, max=1.0, mean=0.5),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m            'avail_actions': np.ndarray((341, 4), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m            'state': np.ndarray((341,), dtype=object, head=Graph(num_nodes={'col': 10, 'row': 100},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m       num_edges={('row', 'elem', 'col'): 1000},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m       metagraph=[('row', 'col', 'elem')]))},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m   'rewards': np.ndarray((341,), dtype=float32, min=-0.001, max=-0.001, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m   'unroll_id': np.ndarray((341,), dtype=int64, min=5.0, max=5.0, mean=5.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m   'value_targets': np.ndarray((341,), dtype=float32, min=-0.214, max=-0.104, mean=-0.132),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m   'vf_preds': np.ndarray((341,), dtype=float32, min=-0.215, max=-0.215, mean=-0.215)}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20331)\u001b[0m 2021-12-30 11:28:49,977\tINFO rollout_worker.py:754 -- Generating sample batch of size 341\n",
      "2021-12-30 11:28:59,657\tDEBUG train_ops.py:190 -- == sgd epochs for default_policy ==\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'episode_reward_max': -1.8455309222842375,\n",
       " 'episode_reward_min': -1.8671428422190117,\n",
       " 'episode_reward_mean': -1.8566294914913752,\n",
       " 'episode_len_mean': 1001.0,\n",
       " 'episode_media': {},\n",
       " 'episodes_this_iter': 3,\n",
       " 'policy_reward_min': {},\n",
       " 'policy_reward_max': {},\n",
       " 'policy_reward_mean': {},\n",
       " 'custom_metrics': {},\n",
       " 'hist_stats': {'episode_reward': [-1.8457294917204952,\n",
       "   -1.8506421356712597,\n",
       "   -1.8455309222842375,\n",
       "   -1.8652934214785981,\n",
       "   -1.8654381355746479,\n",
       "   -1.8671428422190117],\n",
       "  'episode_lengths': [1001, 1001, 1001, 1001, 1001, 1001]},\n",
       " 'sampler_perf': {'mean_raw_obs_processing_ms': 0.25547610235272056,\n",
       "  'mean_inference_ms': 8.731565234282522,\n",
       "  'mean_action_processing_ms': 0.2235897798484958,\n",
       "  'mean_env_wait_ms': 16.234337814841346,\n",
       "  'mean_env_render_ms': 0.0},\n",
       " 'off_policy_estimator': {},\n",
       " 'num_healthy_workers': 3,\n",
       " 'timesteps_total': 6138,\n",
       " 'timesteps_this_iter': 0,\n",
       " 'agent_timesteps_total': 6138,\n",
       " 'timers': {'sample_time_ms': 94623.896,\n",
       "  'sample_throughput': 21.622,\n",
       "  'load_time_ms': 1.066,\n",
       "  'load_throughput': 1919093.399,\n",
       "  'learn_time_ms': 10776.837,\n",
       "  'learn_throughput': 189.852,\n",
       "  'update_time_ms': 4.916},\n",
       " 'info': {'learner': {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.05,\n",
       "     'cur_lr': 5.0000000000000016e-05,\n",
       "     'total_loss': 0.22922955067388506,\n",
       "     'policy_loss': 0.1985646763325551,\n",
       "     'vf_loss': 0.03066486029756359,\n",
       "     'vf_explained_var': 0.047629493872324624,\n",
       "     'kl': 2.842454425936817e-07,\n",
       "     'entropy': 17.725276251898872,\n",
       "     'entropy_coeff': 0.0}}},\n",
       "  'num_steps_sampled': 6138,\n",
       "  'num_agent_steps_sampled': 6138,\n",
       "  'num_steps_trained': 6138,\n",
       "  'num_agent_steps_trained': 6138,\n",
       "  'num_steps_trained_this_iter': 0},\n",
       " 'done': False,\n",
       " 'episodes_total': 6,\n",
       " 'training_iteration': 3,\n",
       " 'trial_id': 'default',\n",
       " 'experiment_id': '59a123dfc49f4299acc1d4418fd1dea8',\n",
       " 'date': '2021-12-30_11-29-10',\n",
       " 'timestamp': 1640863750,\n",
       " 'time_this_iter_s': 31.109976768493652,\n",
       " 'time_total_s': 94.03177809715271,\n",
       " 'pid': 20215,\n",
       " 'hostname': 'OPOVLS001VI',\n",
       " 'node_ip': '10.211.200.166',\n",
       " 'config': {'num_workers': 3,\n",
       "  'num_envs_per_worker': 1,\n",
       "  'create_env_on_driver': False,\n",
       "  'rollout_fragment_length': 341,\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'gamma': 0.99,\n",
       "  'lr': 5e-05,\n",
       "  'train_batch_size': 1024,\n",
       "  'model': {'_use_default_native_models': False,\n",
       "   '_disable_preprocessor_api': True,\n",
       "   'fcnet_hiddens': [256, 256],\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'post_fcnet_hiddens': [],\n",
       "   'post_fcnet_activation': 'relu',\n",
       "   'free_log_std': False,\n",
       "   'no_final_linear': False,\n",
       "   'vf_share_layers': False,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action': False,\n",
       "   'lstm_use_prev_reward': False,\n",
       "   '_time_major': False,\n",
       "   'use_attention': False,\n",
       "   'attention_num_transformer_units': 1,\n",
       "   'attention_dim': 64,\n",
       "   'attention_num_heads': 1,\n",
       "   'attention_head_dim': 32,\n",
       "   'attention_memory_inference': 50,\n",
       "   'attention_memory_training': 50,\n",
       "   'attention_position_wise_mlp_dim': 32,\n",
       "   'attention_init_gru_gate_bias': 2.0,\n",
       "   'attention_use_n_prev_actions': 0,\n",
       "   'attention_use_n_prev_rewards': 0,\n",
       "   'framestack': True,\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': 'general_model_torch',\n",
       "   'custom_model_config': {'fcnet_feats': [256, 256]},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None,\n",
       "   'lstm_use_prev_action_reward': -1},\n",
       "  'optimizer': {},\n",
       "  'horizon': None,\n",
       "  'soft_horizon': False,\n",
       "  'no_done_at_end': False,\n",
       "  'env': 'BiclusterEnv-v0',\n",
       "  'observation_space': None,\n",
       "  'action_space': None,\n",
       "  'env_config': {'shape': [[100, 10], [100, 10]],\n",
       "   'n': 5,\n",
       "   'clusters': [5, 5],\n",
       "   'dataset_settings': {'dstype': {'value': 'Symbolic'},\n",
       "    'patterns': {'value': [['CONSTANT', 'CONSTANT']]},\n",
       "    'symbols': {'value': [-1, 1]},\n",
       "    'bktype': {'value': 'UNIFORM'},\n",
       "    'clusterdistribution': {'value': [['UNIFORM', 8, 12], ['UNIFORM', 4, 6]]},\n",
       "    'contiguity': {'value': None},\n",
       "    'plaidcoherency': {'value': 'NO_OVERLAPPING'}},\n",
       "   'max_steps': 1000,\n",
       "   'clust_init': <function nclustRL.utils.helper.randint(size, dtype)>},\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'env_task_fn': None,\n",
       "  'render_env': False,\n",
       "  'record_env': False,\n",
       "  'clip_rewards': False,\n",
       "  'normalize_actions': True,\n",
       "  'clip_actions': False,\n",
       "  'preprocessor_pref': None,\n",
       "  'log_level': 'DEBUG',\n",
       "  'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       "  'ignore_worker_failures': False,\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'framework': 'torch',\n",
       "  'eager_tracing': False,\n",
       "  'eager_max_retraces': 20,\n",
       "  'explore': False,\n",
       "  'exploration_config': {'type': 'StochasticSampling'},\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_num_episodes': 10,\n",
       "  'evaluation_parallel_to_training': False,\n",
       "  'in_evaluation': False,\n",
       "  'evaluation_config': {'explore': False},\n",
       "  'evaluation_num_workers': 0,\n",
       "  'custom_eval_function': None,\n",
       "  'sample_async': False,\n",
       "  'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'synchronize_filters': True,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'compress_observations': False,\n",
       "  'collect_metrics_timeout': 180,\n",
       "  'metrics_smoothing_episodes': 100,\n",
       "  'min_iter_time_s': 0,\n",
       "  'timesteps_per_iteration': 0,\n",
       "  'seed': None,\n",
       "  'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'num_gpus': 0.0001,\n",
       "  '_fake_gpus': False,\n",
       "  'num_cpus_per_worker': 1,\n",
       "  'num_gpus_per_worker': 0.3333,\n",
       "  'custom_resources_per_worker': {},\n",
       "  'num_cpus_for_driver': 1,\n",
       "  'placement_strategy': 'PACK',\n",
       "  'input': 'sampler',\n",
       "  'input_config': {},\n",
       "  'actions_in_input_normalized': False,\n",
       "  'input_evaluation': ['is', 'wis'],\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.agents.ppo.ppo_torch_policy.PPOTorchPolicy'>, observation_space=None, action_space=None, config={})},\n",
       "   'policy_map_capacity': 100,\n",
       "   'policy_map_cache': None,\n",
       "   'policy_mapping_fn': None,\n",
       "   'policies_to_train': None,\n",
       "   'observation_fn': None,\n",
       "   'replay_mode': 'independent',\n",
       "   'count_steps_by': 'env_steps'},\n",
       "  'logger_config': None,\n",
       "  '_tf_policy_handles_more_than_one_loss': False,\n",
       "  '_disable_preprocessor_api': True,\n",
       "  'simple_optimizer': False,\n",
       "  'monitor': -1,\n",
       "  'use_critic': True,\n",
       "  'use_gae': True,\n",
       "  'lambda': 1.0,\n",
       "  'kl_coeff': 0.2,\n",
       "  'sgd_minibatch_size': 128,\n",
       "  'shuffle_sequences': True,\n",
       "  'num_sgd_iter': 30,\n",
       "  'lr_schedule': None,\n",
       "  'vf_loss_coeff': 1.0,\n",
       "  'entropy_coeff': 0.0,\n",
       "  'entropy_coeff_schedule': None,\n",
       "  'clip_param': 0.3,\n",
       "  'vf_clip_param': 10.0,\n",
       "  'grad_clip': None,\n",
       "  'kl_target': 0.01,\n",
       "  'vf_share_layers': -1},\n",
       " 'time_since_restore': 94.03177809715271,\n",
       " 'timesteps_since_restore': 0,\n",
       " 'iterations_since_restore': 3,\n",
       " 'perf': {'cpu_util_percent': 35.025, 'ram_util_percent': 93.87750000000001}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nclustRL.models import RlModel, GraphEncoder\n",
    "from nclustRL.models.model import FcNet\n",
    "from nclustRL.utils.helper import generate_dummy_obs, transform_obs\n",
    "import nclustenv\n",
    "from nclustenv.configs.biclustering.binary import base\n",
    "from nclustRL.configs.default_configs import MODEL_DEFAULTS\n",
    "from gym.wrappers import TransformObservation\n",
    "\n",
    "import torch as th \n",
    "from ray.rllib.utils.torch_utils import FLOAT_MIN, FLOAT_MAX\n",
    "\n",
    "BATCH = 32\n",
    "\n",
    "def make_env():\n",
    "    return TransformObservation(nclustenv.make('BiclusterEnv-v0', **base), transform_obs)\n",
    "\n",
    "def init_rlmodel():\n",
    "\n",
    "    env = make_env()\n",
    "    \n",
    "    model = RlModel(env.observation_space, env.action_space, 28, MODEL_DEFAULTS, '')\n",
    "    return model.cuda()\n",
    "\n",
    "def generate_dummy_dict():\n",
    "    env = make_env()\n",
    "    return {'obs': [env.reset() for _ in range(BATCH)]}\n",
    "\n",
    "rlmodel = init_rlmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro.cotovio/Repos/nclustRL/nclustRL/models/model.py:144: UserWarning: Forward is running with random obs, this should only be acceptable during policy inicialization.\n",
      "  warnings.warn('Forward is running with random obs, this should only be acceptable during policy inicialization.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = make_env()\n",
    "\n",
    "model = GraphEncoder(env.observation_space['state'].n, [64], ['elem'])\n",
    "model.cuda()\n",
    "obs_flat = rlmodel._generate_dummy_obs(th.zeros((BATCH, 2)))\n",
    "hg = model(obs_flat)\n",
    "hg.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = FcNet(64, [256, 256])\n",
    "model2.cuda()\n",
    "logits = model2(hg)\n",
    "logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 24])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_params = rlmodel._param_branch(logits)\n",
    "_params.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_actions = rlmodel._action_branch(logits)\n",
    "_actions.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rlmodel._value_branch(logits).squeeze(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 28])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "\n",
    "avail_actions = th.tensor(obs[\"avail_actions\"], device=obs_flat.device).unsqueeze(0).repeat(32, 1)\n",
    "action_mask = th.tensor(obs[\"action_mask\"], device=obs_flat.device).unsqueeze(0).repeat(32, 1)\n",
    "\n",
    "intent_vector = th.unsqueeze(_actions, 1)\n",
    "action_logits = th.sum(avail_actions * intent_vector, dim=1)\n",
    "inf_mask = th.clamp(th.log(action_mask), FLOAT_MIN, FLOAT_MAX)\n",
    "\n",
    "th.cat(((action_logits + inf_mask), _params), dim=1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'is_block'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_236088/3686454552.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdgl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Repos/nclustRL/venv/lib/python3.8/site-packages/dgl/batch.py\u001b[0m in \u001b[0;36mbatch\u001b[0;34m(graphs, ndata, edata, node_attrs, edge_attrs)\u001b[0m\n\u001b[1;32m    166\u001b[0m         raise DGLError('Invalid argument edata: must be a string list but got {}.'.format(\n\u001b[1;32m    167\u001b[0m             type(edata)))\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_block\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mDGLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Batching a MFG is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Repos/nclustRL/venv/lib/python3.8/site-packages/dgl/batch.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    166\u001b[0m         raise DGLError('Invalid argument edata: must be a string list but got {}.'.format(\n\u001b[1;32m    167\u001b[0m             type(edata)))\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_block\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mDGLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Batching a MFG is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'is_block'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import dgl\n",
    "dgl.batch(list(np.array([env.reset() for _ in range(10)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes={'col': 100, 'row': 1000},\n",
       "      num_edges={('row', 'elem', 'col'): 10000},\n",
       "      metagraph=[('row', 'col', 'elem')])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgl.batch(list(np.array([env.reset()['state'] for _ in range(10)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
