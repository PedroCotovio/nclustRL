{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents import ppo\n",
    "import ray\n",
    "import torch as th\n",
    "from ray.tune.logger import pretty_print\n",
    "import nclustRL\n",
    "from nclustRL.utils.helper import transform_obs\n",
    "from ray.tune import tune, grid_search\n",
    "from gym.wrappers import TransformObservation\n",
    "import nclustenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ray.init()\n",
    "except RuntimeError:\n",
    "    passgrid_searchdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-special",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = nclustenv.make('BiclusterEnv-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-scottish",
   "metadata": {},
   "outputs": [],
   "source": [
    "env2 = TransformObservation(env, transform_obs)\n",
    "obs = env.reset()['state']\n",
    "obs_flat = env2.reset()['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31854a39-4b22-438b-ab97-5ac407330352",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.ndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a943ca11-269f-478c-8a67-4ae2d18a791a",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_flat.ndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "206ec23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "\n",
    "def transform_obs(n, obs):\n",
    "\n",
    "    nclusters = n\n",
    "\n",
    "    state = obs.clone()\n",
    "    ntypes = state.ntypes\n",
    "\n",
    "    for n, axis in enumerate(ntypes):\n",
    "        for i in range(nclusters):\n",
    "            state.nodes[axis].data[i] = torch.randint(0, 2, (len(state.nodes(axis)),), dtype=torch.bool).to('cuda:0')\n",
    "\n",
    "    keys = sorted(list(state.nodes[ntypes[0]].data.keys()))\n",
    "    ndata = {}\n",
    "\n",
    "    for ntype in ntypes:\n",
    "        ndata[ntype] = torch.vstack(\n",
    "            [state.ndata[key][ntype].float() for key in keys]\n",
    "        ).transpose(0, 1).to('cuda:0')\n",
    "\n",
    "        state.nodes[ntype].data.clear()\n",
    "    state.ndata['feat'] = ndata\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7efd0ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.nn.pytorch as dglnn\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from nclustRL.utils.helper import pairwise\n",
    "\n",
    "class HeteroRelu(nn.ReLU):\n",
    "\n",
    "    def __init__(self, inplace:bool = False):\n",
    "        super(HeteroRelu, self).__init__(inplace=inplace)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        return {k: super(HeteroRelu, self).forward(v) for k, v in inputs.items()}\n",
    "\n",
    "class GraphSequential(nn.Sequential):\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        super(GraphSequential, self).__init__(*args)\n",
    "\n",
    "    def forward(self, graph, feat, edge_weight=None):\n",
    "        for module in self:\n",
    "\n",
    "            if isinstance(module, dglnn.HeteroGraphConv):\n",
    "\n",
    "                rel_names = zip(module.mods.keys(), graph.canonical_etypes)\n",
    "                feat = module(\n",
    "                    g=graph, \n",
    "                    inputs=feat, \n",
    "                    mod_kwargs={\n",
    "                        rel: dict(edge_weight=graph.edges[canonical].data[edge_weight]) \n",
    "                        for rel, canonical in rel_names})\n",
    "\n",
    "            else:\n",
    "                feat = module(inputs=feat)\n",
    "\n",
    "        return feat\n",
    "\n",
    "\n",
    "class RGCN(nn.Module):\n",
    "    def __init__(self, layers, rel_names):\n",
    "        super().__init__()\n",
    "\n",
    "        _layers = []\n",
    "\n",
    "        for in_feats, out_feats in pairwise(layers): \n",
    "\n",
    "            _layers.append(dglnn.HeteroGraphConv({\n",
    "                rel: dglnn.GraphConv(in_feats, out_feats)\n",
    "                for rel in rel_names}, aggregate='sum'))\n",
    "\n",
    "            _layers.append(HeteroRelu())\n",
    "\n",
    "        self._hidden_layers = GraphSequential(*_layers)\n",
    "\n",
    "            \n",
    "\n",
    "    def forward(self, graph, feat, edge_weight=None):\n",
    "\n",
    "        return self._hidden_layers(graph, feat, edge_weight)\n",
    "\n",
    "\n",
    "class GraphEncoder(nn.Module):\n",
    "    def __init__(self, n, conv_feats, n_classes, rel_names):\n",
    "        super().__init__()\n",
    "\n",
    "        conv_feats.insert(0, n)\n",
    "        self.rgcn = RGCN(conv_feats, rel_names)\n",
    "\n",
    "    def forward(self, g):\n",
    "        h = g.ndata['feat']\n",
    "        h = self.rgcn(g, h, 'w')\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            hg = 0\n",
    "            for ntype in h.keys():\n",
    "                hg = hg + dgl.mean_nodes(g, 'h', ntype=ntype)\n",
    "            \n",
    "            return hg\n",
    "\n",
    "\n",
    "class HeteroClassifier(nn.Module):\n",
    "    def __init__(self, n, conv_feats, n_classes, rel_names):\n",
    "        super().__init__()\n",
    "\n",
    "        conv_feats.insert(0, n)\n",
    "\n",
    "        self.rgcn = RGCN(conv_feats, rel_names)\n",
    "        self.classify = nn.Linear(conv_feats[-1], n_classes)\n",
    "\n",
    "    def forward(self, g):\n",
    "        h = g.ndata['feat']\n",
    "        h = self.rgcn(g, h, 'w')\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            hg = 0\n",
    "            for ntype in h.keys():\n",
    "                hg = hg + dgl.mean_nodes(g, 'h', ntype=ntype)\n",
    "\n",
    "            return self.classify(hg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4f557fb1-c31e-4a7f-b621-900f3638899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "\n",
    "dgl.seed(5)\n",
    "\n",
    "def test_embedings(graphs):\n",
    "\n",
    "    batch_size=1\n",
    "    shuffle=True\n",
    "    nclasses = 5\n",
    "    n = 5\n",
    "\n",
    "    # dataloader = GraphDataLoader(\n",
    "    #     base,\n",
    "    #     batch_size=batch_size,\n",
    "    #     drop_last=False,\n",
    "    #     shuffle=shuffle)\n",
    "\n",
    "    etypes = graphs[0].etypes\n",
    "\n",
    "    model = HeteroClassifier(n, [n*2], nclasses, etypes)\n",
    "    model = model.cuda()\n",
    "    opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "    for epoch in range(20):\n",
    "        with tqdm(graphs, unit=\"batch\") as tepoch:\n",
    "            for batched_graph in tepoch:\n",
    "\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "                # batched_graph = transform_obs(n, batched_graph)\n",
    "                labels = torch.randint(0, 4, (batch_size,)).to('cuda:0')\n",
    "\n",
    "                logits = model(batched_graph)\n",
    "                loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "                predictions = logits.argmax(dim=1, keepdim=True).squeeze()\n",
    "                correct = (logits == labels).sum().item()\n",
    "\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "                accuracy = correct / batch_size\n",
    "                tepoch.set_postfix(loss=loss.item(), accuracy=100. * accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e3bca980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 10/10 [00:00<00:00, 210.81batch/s, accuracy=0, loss=1.65]\n",
      "Epoch 1: 100%|██████████| 10/10 [00:00<00:00, 219.78batch/s, accuracy=0, loss=1.36]\n",
      "Epoch 2: 100%|██████████| 10/10 [00:00<00:00, 152.42batch/s, accuracy=0, loss=1.49]\n",
      "Epoch 3: 100%|██████████| 10/10 [00:00<00:00, 181.30batch/s, accuracy=0, loss=1.57]\n",
      "Epoch 4: 100%|██████████| 10/10 [00:00<00:00, 114.72batch/s, accuracy=0, loss=1.48]\n",
      "Epoch 5: 100%|██████████| 10/10 [00:00<00:00, 131.34batch/s, accuracy=0, loss=1.32]\n",
      "Epoch 6: 100%|██████████| 10/10 [00:00<00:00, 115.82batch/s, accuracy=0, loss=1.74]\n",
      "Epoch 7: 100%|██████████| 10/10 [00:00<00:00, 142.62batch/s, accuracy=0, loss=1.69]\n",
      "Epoch 8: 100%|██████████| 10/10 [00:00<00:00, 198.99batch/s, accuracy=0, loss=1.33]\n",
      "Epoch 9: 100%|██████████| 10/10 [00:00<00:00, 186.54batch/s, accuracy=0, loss=1.63]\n",
      "Epoch 10: 100%|██████████| 10/10 [00:00<00:00, 192.31batch/s, accuracy=0, loss=1.62]\n",
      "Epoch 11: 100%|██████████| 10/10 [00:00<00:00, 220.83batch/s, accuracy=0, loss=1.61]\n",
      "Epoch 12: 100%|██████████| 10/10 [00:00<00:00, 245.56batch/s, accuracy=0, loss=1.59]\n",
      "Epoch 13: 100%|██████████| 10/10 [00:00<00:00, 210.51batch/s, accuracy=0, loss=1.48]\n",
      "Epoch 14: 100%|██████████| 10/10 [00:00<00:00, 202.24batch/s, accuracy=0, loss=1.56]\n",
      "Epoch 15: 100%|██████████| 10/10 [00:00<00:00, 237.43batch/s, accuracy=0, loss=1.45]\n",
      "Epoch 16: 100%|██████████| 10/10 [00:00<00:00, 255.06batch/s, accuracy=0, loss=1.35]\n",
      "Epoch 17: 100%|██████████| 10/10 [00:00<00:00, 200.94batch/s, accuracy=0, loss=1.44]\n",
      "Epoch 18: 100%|██████████| 10/10 [00:00<00:00, 266.31batch/s, accuracy=0, loss=1.33]\n",
      "Epoch 19: 100%|██████████| 10/10 [00:00<00:00, 186.91batch/s, accuracy=0, loss=1.55]\n"
     ]
    }
   ],
   "source": [
    "test_embedings(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0cfc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor([[0.0000, 1.4752, 3.0812, 1.3285, 0.4708, 0.0000, 0.9958, 0.0000, 2.8396,\n",
    "         2.7786]], device='cuda:0', grad_fn=<AddBackward0>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7223bf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    import torch as th\n",
    "    import dgl\n",
    "    def loader(cls, module=None):\n",
    "\n",
    "        return getattr(module, cls) if isinstance(cls, str) else cls\n",
    "    \n",
    "    def dense_to_dgl(x, device, cuda=0, nclusters=1, clust_init='zeros', duplicate=True):\n",
    "\n",
    "        # set (u,v)\n",
    "        clust_init = loader(th, clust_init)\n",
    "\n",
    "        tensor = th.tensor([[i, j, elem] for i, row in enumerate(x) for j, elem in enumerate(row)]).T\n",
    "\n",
    "        if duplicate:\n",
    "\n",
    "            graph_data = {\n",
    "                ('row', 'elem', 'col'): (tensor[0].int(), tensor[1].int()),\n",
    "                ('col', 'elem', 'row'): (tensor[1].int().detach().clone(), tensor[2].int().detach().clone()),\n",
    "                }\n",
    "\n",
    "            # create graph\n",
    "            G = dgl.heterograph(graph_data)\n",
    "\n",
    "            # set weights\n",
    "            G.edges[('row', 'elem', 'col')].data['w'] = tensor[2].float()\n",
    "            G.edges[('col', 'elem', 'row')].data['w'] = tensor[2].float()\n",
    "\n",
    "        else:\n",
    "\n",
    "            graph_data = {\n",
    "                ('row', 'elem', 'col'): (tensor[0].int(), tensor[1].int()),\n",
    "                }\n",
    "\n",
    "            # create graph\n",
    "            G = dgl.heterograph(graph_data)\n",
    "\n",
    "            # set weights\n",
    "            G.edges[('row', 'elem', 'col')].data['w'] = tensor[2].float()\n",
    "\n",
    "        # set cluster members\n",
    "\n",
    "        for n, axis in enumerate(['row', 'col']):\n",
    "            for i in range(nclusters):\n",
    "                G.nodes[axis].data[i] = th.randint(0, 2, (x.shape[n],), dtype=torch.bool)\n",
    "\n",
    "        ndata = {}\n",
    "        ntypes = G.ntypes\n",
    "        keys = sorted(list(G.nodes[ntypes[0]].data.keys()))\n",
    "\n",
    "        for ntype in ntypes:\n",
    "            ndata[ntype] = torch.vstack(\n",
    "                [G.ndata[key][ntype].float() for key in keys]\n",
    "            ).transpose(0, 1)\n",
    "\n",
    "            G.nodes[ntype].data.clear()\n",
    "\n",
    "        G.ndata['feat'] = ndata\n",
    "\n",
    "        if device == 'gpu':\n",
    "            G = G.to('cuda:{}'.format(cuda))\n",
    "\n",
    "        return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5db59c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nclustenv\n",
    "import torch\n",
    "env = nclustenv.make('BiclusterEnv-v0', **dict(shape=[[100, 10], [110, 15]], clusters=[5,5]))\n",
    "\n",
    "graphs_dup = []\n",
    "graphs = []\n",
    "for i in range(10):\n",
    "    env.reset()\n",
    "    X = env.state._generator.X\n",
    "    graphs_dup.append(dense_to_dgl(X, device='gpu', nclusters=5))\n",
    "    graphs.append(dense_to_dgl(X, device='gpu', nclusters=5, duplicate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "51139c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes={'col': 14, 'row': 104},\n",
       "      num_edges={('col', 'elem', 'row'): 1456, ('row', 'elem', 'col'): 1456},\n",
       "      metagraph=[('col', 'row', 'elem'), ('row', 'col', 'elem')])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphs_dup[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "418c0273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes={'col': 14, 'row': 104},\n",
       "      num_edges={('row', 'elem', 'col'): 1456},\n",
       "      metagraph=[('row', 'col', 'elem')])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885297ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f057d67ad8388b0f5d50624cd1d42e8685c722b60a2c37b21ef81e4231f602cb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
