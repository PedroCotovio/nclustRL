{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicilaze trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from nclustRL.trainer import Trainer\n",
    "from nclustRL.configs.default_configs import PPO_PBT, DEFAULT_CONFIG\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from nclustenv.configs import biclustering, triclustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-28 17:51:30,027\tWARNING ppo.py:143 -- `train_batch_size` (1024) cannot be achieved with your other settings (num_workers=3 num_envs_per_worker=1 rollout_fragment_length=341)! Auto-adjusting `rollout_fragment_length` to 341.\n",
      "2022-02-28 17:51:30,028\tINFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14541)\u001b[0m Using backend: pytorch\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14545)\u001b[0m Using backend: pytorch\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14542)\u001b[0m Using backend: pytorch\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14541)\u001b[0m 2022-02-28 17:51:35,565\tINFO worker.py:852 -- Calling ray.init() again after it has already been called.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14542)\u001b[0m 2022-02-28 17:51:35,564\tINFO worker.py:852 -- Calling ray.init() again after it has already been called.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14545)\u001b[0m 2022-02-28 17:51:35,778\tINFO worker.py:852 -- Calling ray.init() again after it has already been called.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14541)\u001b[0m 2022-02-28 17:51:46,911\tINFO rollout_worker.py:1705 -- Validating sub-env at vector index=0 ... (ok)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14541)\u001b[0m 2022-02-28 17:51:46,911\tDEBUG rollout_worker.py:1534 -- Creating policy for default_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14541)\u001b[0m 2022-02-28 17:51:46,915\tINFO catalog.py:410 -- Wrapping <class 'nclustRL.models.model.RlModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14545)\u001b[0m 2022-02-28 17:51:46,909\tINFO rollout_worker.py:1705 -- Validating sub-env at vector index=0 ... (ok)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14545)\u001b[0m 2022-02-28 17:51:46,910\tDEBUG rollout_worker.py:1534 -- Creating policy for default_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14545)\u001b[0m 2022-02-28 17:51:46,916\tINFO catalog.py:410 -- Wrapping <class 'nclustRL.models.model.RlModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14542)\u001b[0m 2022-02-28 17:51:46,911\tINFO rollout_worker.py:1705 -- Validating sub-env at vector index=0 ... (ok)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14542)\u001b[0m 2022-02-28 17:51:46,911\tDEBUG rollout_worker.py:1534 -- Creating policy for default_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14542)\u001b[0m 2022-02-28 17:51:46,917\tINFO catalog.py:410 -- Wrapping <class 'nclustRL.models.model.RlModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14541)\u001b[0m 2022-02-28 17:51:47,077\tINFO torch_policy.py:186 -- TorchPolicy (worker=3) running on 0.3333 GPU(s).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14541)\u001b[0m /home/pedro.cotovio/Repos/nclustRL/nclustRL/models/model.py:147: UserWarning: Forward is running with random obs, this should only be acceptable during policy inicialization.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14541)\u001b[0m   warnings.warn('Forward is running with random obs, this should only be acceptable during policy inicialization.')\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14545)\u001b[0m 2022-02-28 17:51:47,077\tINFO torch_policy.py:186 -- TorchPolicy (worker=1) running on 0.3333 GPU(s).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14545)\u001b[0m /home/pedro.cotovio/Repos/nclustRL/nclustRL/models/model.py:147: UserWarning: Forward is running with random obs, this should only be acceptable during policy inicialization.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14545)\u001b[0m   warnings.warn('Forward is running with random obs, this should only be acceptable during policy inicialization.')\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14542)\u001b[0m 2022-02-28 17:51:47,077\tINFO torch_policy.py:186 -- TorchPolicy (worker=2) running on 0.3333 GPU(s).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14542)\u001b[0m /home/pedro.cotovio/Repos/nclustRL/nclustRL/models/model.py:147: UserWarning: Forward is running with random obs, this should only be acceptable during policy inicialization.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14542)\u001b[0m   warnings.warn('Forward is running with random obs, this should only be acceptable during policy inicialization.')\n",
      "2022-02-28 17:51:52,455\tINFO worker_set.py:104 -- Inferred observation/action spaces from remote worker (local worker has no env): {'default_policy': (Dict(action_mask:Box([0. 0. 0. 0.], [1. 1. 1. 1.], (4,), float32), avail_actions:Box([0. 0. 0. 0.], [1. 1. 1. 1.], (4,), float32), state:Box([100  10], [100  10], (2,), int32)), Tuple(Discrete(4), Tuple(Box([0. 0. 0.], [1. 1. 1.], (3,), float32), Box([0. 0. 0.], [1. 1. 1.], (3,), float32), Box([0. 0. 0.], [1. 1. 1.], (3,), float32), Box([0. 0. 0.], [1. 1. 1.], (3,), float32)))), '__env__': (Dict(action_mask:Box([0. 0. 0. 0.], [1. 1. 1. 1.], (4,), float32), avail_actions:Box([0. 0. 0. 0.], [1. 1. 1. 1.], (4,), float32), state:Box([100  10], [100  10], (2,), int32)), Tuple(Discrete(4), Tuple(Box([0. 0. 0.], [1. 1. 1.], (3,), float32), Box([0. 0. 0.], [1. 1. 1.], (3,), float32), Box([0. 0. 0.], [1. 1. 1.], (3,), float32), Box([0. 0. 0.], [1. 1. 1.], (3,), float32))))}\n",
      "2022-02-28 17:51:52,456\tDEBUG rollout_worker.py:1534 -- Creating policy for default_policy\n",
      "2022-02-28 17:51:52,459\tINFO catalog.py:410 -- Wrapping <class 'nclustRL.models.model.RlModel'> as None\n",
      "2022-02-28 17:51:52,464\tINFO torch_policy.py:186 -- TorchPolicy (worker=local) running on 0.0001 GPU(s).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14545)\u001b[0m 2022-02-28 17:51:52,437\tDEBUG rollout_worker.py:728 -- Created rollout worker with env <ray.rllib.env.base_env._VectorEnvToBaseEnv object at 0x7fa890096730> (<TransformObservation<OrderEnforcing<BiclusterEnv<BiclusterEnv-v0>>>>), policies {}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14541)\u001b[0m 2022-02-28 17:51:52,530\tDEBUG rollout_worker.py:728 -- Created rollout worker with env <ray.rllib.env.base_env._VectorEnvToBaseEnv object at 0x7ff052cfa280> (<TransformObservation<OrderEnforcing<BiclusterEnv<BiclusterEnv-v0>>>>), policies {}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14542)\u001b[0m 2022-02-28 17:51:52,535\tDEBUG rollout_worker.py:728 -- Created rollout worker with env <ray.rllib.env.base_env._VectorEnvToBaseEnv object at 0x7fb9b6d84be0> (<TransformObservation<OrderEnforcing<BiclusterEnv<BiclusterEnv-v0>>>>), policies {}\n",
      "/home/pedro.cotovio/Repos/nclustRL/nclustRL/models/model.py:147: UserWarning: Forward is running with random obs, this should only be acceptable during policy inicialization.\n",
      "  warnings.warn('Forward is running with random obs, this should only be acceptable during policy inicialization.')\n",
      "2022-02-28 17:52:08,888\tINFO rollout_worker.py:1555 -- Built policy map: {}\n",
      "2022-02-28 17:52:08,899\tINFO rollout_worker.py:1556 -- Built preprocessor map: {'default_policy': None}\n",
      "2022-02-28 17:52:08,900\tINFO rollout_worker.py:618 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7faacafbd520>}\n",
      "2022-02-28 17:52:08,901\tDEBUG rollout_worker.py:728 -- Created rollout worker with env None (None), policies {}\n",
      "2022-02-28 17:52:09,064\tINFO trainable.py:124 -- Trainable.setup took 39.038 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-02-28 17:52:09,115\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    trainer=PPOTrainer,\n",
    "    env='BiclusterEnv-v0',\n",
    "    save_dir='/home/pedro.cotovio/Repos/nclustRL/Exp/Exp1'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets calculate the average metrics for this random agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 of 10 done.\n",
      "Episode 2 of 10 done.\n",
      "Episode 3 of 10 done.\n",
      "Episode 4 of 10 done.\n",
      "Episode 5 of 10 done.\n",
      "Episode 6 of 10 done.\n",
      "Episode 7 of 10 done.\n",
      "Episode 8 of 10 done.\n",
      "Episode 9 of 10 done.\n",
      "Episode 10 of 10 done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-1.9985294117647068, 0.00431907308377899)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-02-23 20:11:46 (running for 00:59:37.68)<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/1 GPUs, 0.0/7.65 GiB heap, 0.0/3.82 GiB objects (0.0/1.0 accelerator_type:T4)<br>Current best trial: 7ed8e_00000 with episode_reward_mean=-1.8429356827096706 and parameters={'num_workers': 3, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 341, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 1024, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': True, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': 'general_model_torch', 'custom_model_config': {'fcnet_feats': [256, 256]}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'BiclusterEnv-v0', 'observation_space': None, 'action_space': None, 'env_config': {'shape': [[100, 10], [100, 10]], 'n': 5, 'clusters': [5, 5], 'dataset_settings': {'dstype': {'value': 'Symbolic'}, 'patterns': {'value': [['CONSTANT', 'CONSTANT']]}, 'symbols': {'value': [-1, 1]}, 'bktype': {'value': 'UNIFORM'}, 'clusterdistribution': {'value': [['UNIFORM', 8, 12], ['UNIFORM', 4, 6]]}, 'contiguity': {'value': None}, 'plaidcoherency': {'value': 'NO_OVERLAPPING'}}, 'max_steps': 1000, 'seed': 537}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': False, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': None, 'log_level': 'DEBUG', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'torch', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'explore': False}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': 537, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0.0001, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.3333, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.agents.ppo.ppo_torch_policy.PPOTorchPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /home/pedro.cotovio/Repos/nclustRL/Exp/Exp1/test/sample_0/PPO_2022-02-23_19-12-08<br>Number of trials: 1/1 (1 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 20:11:46,389\tINFO tune.py:626 -- Total run time: 3578.05 seconds (3577.64 seconds for the tuning loop).\n",
      "Sample 1: : 1sample [59:38, 3578.29s/sample, metric=-1.84]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'config': {'num_workers': 3,\n",
       "  'create_env_on_driver': False,\n",
       "  'num_envs_per_worker': 1,\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'gamma': 0.99,\n",
       "  'use_critic': True,\n",
       "  'use_gae': True,\n",
       "  'lambda': 1.0,\n",
       "  'kl_coeff': 0.2,\n",
       "  'rollout_fragment_length': 341,\n",
       "  'train_batch_size': 1024,\n",
       "  'sgd_minibatch_size': 128,\n",
       "  'shuffle_sequences': True,\n",
       "  'num_sgd_iter': 30,\n",
       "  'lr': 5e-05,\n",
       "  'lr_schedule': None,\n",
       "  'vf_loss_coeff': 1.0,\n",
       "  'entropy_coeff': 0.0,\n",
       "  'entropy_coeff_schedule': None,\n",
       "  'clip_param': 0.3,\n",
       "  'vf_clip_param': 10.0,\n",
       "  'grad_clip': None,\n",
       "  'kl_target': 0.01,\n",
       "  'optimizer': {},\n",
       "  'horizon': None,\n",
       "  'soft_horizon': False,\n",
       "  'no_done_at_end': False,\n",
       "  'env': 'BiclusterEnv-v0',\n",
       "  'observation_space': None,\n",
       "  'action_space': None,\n",
       "  'env_config': {'shape': [[100, 10], [100, 10]],\n",
       "   'n': 5,\n",
       "   'clusters': [5, 5],\n",
       "   'dataset_settings': {'dstype': {'value': 'Symbolic'},\n",
       "    'patterns': {'value': [['CONSTANT', 'CONSTANT']]},\n",
       "    'symbols': {'value': [-1, 1]},\n",
       "    'bktype': {'value': 'UNIFORM'},\n",
       "    'clusterdistribution': {'value': [['UNIFORM', 8, 12], ['UNIFORM', 4, 6]]},\n",
       "    'contiguity': {'value': None},\n",
       "    'plaidcoherency': {'value': 'NO_OVERLAPPING'}},\n",
       "   'max_steps': 1000,\n",
       "   'seed': 537},\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'env_task_fn': None,\n",
       "  'render_env': False,\n",
       "  'record_env': False,\n",
       "  'clip_rewards': False,\n",
       "  'normalize_actions': True,\n",
       "  'preprocessor_pref': None,\n",
       "  'log_level': 'DEBUG',\n",
       "  'ignore_worker_failures': False,\n",
       "  'log_sys_usage': True,\n",
       "  'framework': 'torch',\n",
       "  'eager_tracing': False,\n",
       "  'explore': True,\n",
       "  'exploration_config': {'type': 'StochasticSampling'},\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_num_episodes': 10,\n",
       "  'evaluation_parallel_to_training': False,\n",
       "  'in_evaluation': False,\n",
       "  'evaluation_config': {'explore': False},\n",
       "  'evaluation_num_workers': 0,\n",
       "  'custom_eval_function': None,\n",
       "  'sample_async': False,\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'synchronize_filters': True,\n",
       "  'compress_observations': False,\n",
       "  'collect_metrics_timeout': 180,\n",
       "  'metrics_smoothing_episodes': 100,\n",
       "  'min_iter_time_s': 0,\n",
       "  'timesteps_per_iteration': 0,\n",
       "  'seed': 537,\n",
       "  'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'num_gpus': 0.0001,\n",
       "  '_fake_gpus': False,\n",
       "  'num_cpus_per_worker': 1,\n",
       "  'num_gpus_per_worker': 0.3333,\n",
       "  'custom_resources_per_worker': {},\n",
       "  'num_cpus_for_driver': 1,\n",
       "  'placement_strategy': 'PACK',\n",
       "  'logger_config': None,\n",
       "  '_disable_preprocessor_api': True,\n",
       "  'model': {'custom_model': 'general_model_torch',\n",
       "   'custom_model_config': {'fcnet_feats': [256, 256]},\n",
       "   'custom_action_dist': None}},\n",
       " 'path': '/home/pedro.cotovio/Repos/nclustRL/Exp/Exp1/test/sample_0/PPO_2022-02-23_19-12-08/PPO_BiclusterEnv-v0_7ed8e_00000_0_2022-02-23_19-12-08/checkpoint_000010/checkpoint-10',\n",
       " 'metric': -1.839142693014666,\n",
       " 'df':    episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
       " 0           -1.800885           -1.887845            -1.842936   \n",
       " \n",
       "    episode_len_mean  episodes_this_iter  num_healthy_workers  timesteps_total  \\\n",
       " 0            1001.0                   3                    3           204600   \n",
       " \n",
       "    timesteps_this_iter  agent_timesteps_total  done  ...  \\\n",
       " 0                    0                 204600  True  ...   \n",
       " \n",
       "    config/shuffle_sequences  config/soft_horizon config/synchronize_filters  \\\n",
       " 0                      True                False                       True   \n",
       " \n",
       "   config/timesteps_per_iteration config/train_batch_size  config/use_critic  \\\n",
       " 0                              0                    1024               True   \n",
       " \n",
       "    config/use_gae  config/vf_clip_param  config/vf_loss_coeff  \\\n",
       " 0            True                  10.0                   1.0   \n",
       " \n",
       "                                               logdir  \n",
       " 0  /home/pedro.cotovio/Repos/nclustRL/Exp/Exp1/te...  \n",
       " \n",
       " [1 rows x 123 columns]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_checkpoint = trainer.train()\n",
    "best_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First results on default configs (Bic-Bin_Base) don't show promise, agent does not seem to be learning the task.\n",
    "\n",
    "The results will now be loaded for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-28 18:06:23,481\tINFO trainable.py:467 -- Restored on 10.211.200.158 from checkpoint: /home/pedro.cotovio/Repos/nclustRL/Exp/Exp1/test/sample_0/PPO_2022-02-23_19-12-08/PPO_BiclusterEnv-v0_7ed8e_00000_0_2022-02-23_19-12-08/checkpoint_000010/checkpoint-10\n",
      "2022-02-28 18:06:23,482\tINFO trainable.py:475 -- Current state after restoring: {'_iteration': 10, '_timesteps_total': 0, '_time_total': 371.6181707382202, '_episodes_total': 18}\n"
     ]
    }
   ],
   "source": [
    "trainer.load('/home/pedro.cotovio/Repos/nclustRL/Exp/Exp1/test/sample_0/PPO_2022-02-23_19-12-08/PPO_BiclusterEnv-v0_7ed8e_00000_0_2022-02-23_19-12-08/checkpoint_000010/checkpoint-10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on 100 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 of 100 done.\n",
      "Episode 2 of 100 done.\n",
      "Episode 3 of 100 done.\n",
      "Episode 4 of 100 done.\n",
      "Episode 5 of 100 done.\n",
      "Episode 6 of 100 done.\n",
      "Episode 7 of 100 done.\n",
      "Episode 8 of 100 done.\n",
      "Episode 9 of 100 done.\n",
      "Episode 10 of 100 done.\n",
      "Episode 11 of 100 done.\n",
      "Episode 12 of 100 done.\n",
      "Episode 13 of 100 done.\n",
      "Episode 14 of 100 done.\n",
      "Episode 15 of 100 done.\n",
      "Episode 16 of 100 done.\n",
      "Episode 17 of 100 done.\n",
      "Episode 18 of 100 done.\n",
      "Episode 19 of 100 done.\n",
      "Episode 20 of 100 done.\n",
      "Episode 21 of 100 done.\n",
      "Episode 22 of 100 done.\n",
      "Episode 23 of 100 done.\n",
      "Episode 24 of 100 done.\n",
      "Episode 25 of 100 done.\n",
      "Episode 26 of 100 done.\n",
      "Episode 27 of 100 done.\n",
      "Episode 28 of 100 done.\n",
      "Episode 29 of 100 done.\n",
      "Episode 30 of 100 done.\n",
      "Episode 31 of 100 done.\n",
      "Episode 32 of 100 done.\n",
      "Episode 33 of 100 done.\n",
      "Episode 34 of 100 done.\n",
      "Episode 35 of 100 done.\n",
      "Episode 36 of 100 done.\n",
      "Episode 37 of 100 done.\n",
      "Episode 38 of 100 done.\n",
      "Episode 39 of 100 done.\n",
      "Episode 40 of 100 done.\n",
      "Episode 41 of 100 done.\n",
      "Episode 42 of 100 done.\n",
      "Episode 43 of 100 done.\n",
      "Episode 44 of 100 done.\n",
      "Episode 45 of 100 done.\n",
      "Episode 46 of 100 done.\n",
      "Episode 47 of 100 done.\n",
      "Episode 48 of 100 done.\n",
      "Episode 49 of 100 done.\n",
      "Episode 50 of 100 done.\n",
      "Episode 51 of 100 done.\n",
      "Episode 52 of 100 done.\n",
      "Episode 53 of 100 done.\n",
      "Episode 54 of 100 done.\n",
      "Episode 55 of 100 done.\n",
      "Episode 56 of 100 done.\n",
      "Episode 57 of 100 done.\n",
      "Episode 58 of 100 done.\n",
      "Episode 59 of 100 done.\n",
      "Episode 60 of 100 done.\n",
      "Episode 61 of 100 done.\n",
      "Episode 62 of 100 done.\n",
      "Episode 63 of 100 done.\n",
      "Episode 64 of 100 done.\n",
      "Episode 65 of 100 done.\n",
      "Episode 66 of 100 done.\n",
      "Episode 67 of 100 done.\n",
      "Episode 68 of 100 done.\n",
      "Episode 69 of 100 done.\n",
      "Episode 70 of 100 done.\n",
      "Episode 71 of 100 done.\n",
      "Episode 72 of 100 done.\n",
      "Episode 73 of 100 done.\n",
      "Episode 74 of 100 done.\n",
      "Episode 75 of 100 done.\n",
      "Episode 76 of 100 done.\n",
      "Episode 77 of 100 done.\n",
      "Episode 78 of 100 done.\n",
      "Episode 79 of 100 done.\n",
      "Episode 80 of 100 done.\n",
      "Episode 81 of 100 done.\n",
      "Episode 82 of 100 done.\n",
      "Episode 83 of 100 done.\n",
      "Episode 84 of 100 done.\n",
      "Episode 85 of 100 done.\n",
      "Episode 86 of 100 done.\n",
      "Episode 87 of 100 done.\n",
      "Episode 88 of 100 done.\n",
      "Episode 89 of 100 done.\n",
      "Episode 90 of 100 done.\n",
      "Episode 91 of 100 done.\n",
      "Episode 92 of 100 done.\n",
      "Episode 93 of 100 done.\n",
      "Episode 94 of 100 done.\n",
      "Episode 95 of 100 done.\n",
      "Episode 96 of 100 done.\n",
      "Episode 97 of 100 done.\n",
      "Episode 98 of 100 done.\n",
      "Episode 99 of 100 done.\n",
      "Episode 100 of 100 done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-1.9915696301312178, 0.008430369868782941)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results clearly don't show learning when compared to the random baseline.\n",
    "Let's try to start with a simpler task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-03 01:58:36,810\tWARNING ppo.py:143 -- `train_batch_size` (1024) cannot be achieved with your other settings (num_workers=3 num_envs_per_worker=1 rollout_fragment_length=341)! Auto-adjusting `rollout_fragment_length` to 341.\n",
      "2022-03-03 01:58:36,811\tINFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=196113)\u001b[0m Using backend: pytorch\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=196112)\u001b[0m Using backend: pytorch\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=196110)\u001b[0m Using backend: pytorch\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=196110)\u001b[0m 2022-03-03 01:58:43,145\tINFO worker.py:852 -- Calling ray.init() again after it has already been called.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=196113)\u001b[0m 2022-03-03 01:58:43,250\tINFO worker.py:852 -- Calling ray.init() again after it has already been called.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=196112)\u001b[0m 2022-03-03 01:58:43,456\tINFO worker.py:852 -- Calling ray.init() again after it has already been called.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=196113)\u001b[0m 2022-03-03 01:58:52,945\tINFO rollout_worker.py:1705 -- Validating sub-env at vector index=0 ... (ok)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=196113)\u001b[0m 2022-03-03 01:58:52,945\tDEBUG rollout_worker.py:1534 -- Creating policy for default_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=196110)\u001b[0m 2022-03-03 01:58:52,891\tINFO rollout_worker.py:1705 -- Validating sub-env at vector index=0 ... (ok)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=196110)\u001b[0m 2022-03-03 01:58:52,891\tDEBUG rollout_worker.py:1534 -- Creating policy for default_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=196112)\u001b[0m 2022-03-03 01:58:53,059\tINFO rollout_worker.py:1705 -- Validating sub-env at vector index=0 ... (ok)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=196112)\u001b[0m 2022-03-03 01:58:53,060\tDEBUG rollout_worker.py:1534 -- Creating policy for default_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=196110)\u001b[0m 2022-03-03 01:58:53,076\tINFO catalog.py:410 -- Wrapping <class 'nclustRL.models.model.RlModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=196113)\u001b[0m 2022-03-03 01:58:53,138\tINFO catalog.py:410 -- Wrapping <class 'nclustRL.models.model.RlModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=196113)\u001b[0m 2022-03-03 01:58:53,141\tINFO torch_policy.py:186 -- TorchPolicy (worker=1) running on 0.3333 GPU(s).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=196113)\u001b[0m /home/pedro.cotovio/Repos/nclustRL/nclustRL/models/model.py:147: UserWarning: Forward is running with random obs, this should only be acceptable during policy inicialization.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=196113)\u001b[0m   warnings.warn('Forward is running with random obs, this should only be acceptable during policy inicialization.')\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=196112)\u001b[0m 2022-03-03 01:58:53,101\tINFO catalog.py:410 -- Wrapping <class 'nclustRL.models.model.RlModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=196110)\u001b[0m 2022-03-03 01:58:53,133\tINFO torch_policy.py:186 -- TorchPolicy (worker=2) running on 0.3333 GPU(s).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=196110)\u001b[0m /home/pedro.cotovio/Repos/nclustRL/nclustRL/models/model.py:147: UserWarning: Forward is running with random obs, this should only be acceptable during policy inicialization.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=196110)\u001b[0m   warnings.warn('Forward is running with random obs, this should only be acceptable during policy inicialization.')\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=196112)\u001b[0m 2022-03-03 01:58:53,222\tINFO torch_policy.py:186 -- TorchPolicy (worker=3) running on 0.3333 GPU(s).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=196112)\u001b[0m /home/pedro.cotovio/Repos/nclustRL/nclustRL/models/model.py:147: UserWarning: Forward is running with random obs, this should only be acceptable during policy inicialization.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=196112)\u001b[0m   warnings.warn('Forward is running with random obs, this should only be acceptable during policy inicialization.')\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=196112)\u001b[0m 2022-03-03 01:58:57,716\tDEBUG rollout_worker.py:728 -- Created rollout worker with env <ray.rllib.env.base_env._VectorEnvToBaseEnv object at 0x7f2af530e070> (<TransformObservation<OrderEnforcing<BiclusterEnv<BiclusterEnv-v0>>>>), policies {}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=196110)\u001b[0m 2022-03-03 01:58:57,679\tDEBUG rollout_worker.py:728 -- Created rollout worker with env <ray.rllib.env.base_env._VectorEnvToBaseEnv object at 0x7fd833cc27f0> (<TransformObservation<OrderEnforcing<BiclusterEnv<BiclusterEnv-v0>>>>), policies {}\n",
      "2022-03-03 01:58:57,771\tINFO worker_set.py:104 -- Inferred observation/action spaces from remote worker (local worker has no env): {'default_policy': (Dict(action_mask:Box([0. 0. 0. 0.], [1. 1. 1. 1.], (4,), float32), avail_actions:Box([0. 0. 0. 0.], [1. 1. 1. 1.], (4,), float32), state:Box([10 10], [10 10], (2,), int32)), Tuple(Discrete(4), Tuple(Box([0. 0. 0.], [1. 1. 1.], (3,), float32), Box([0. 0. 0.], [1. 1. 1.], (3,), float32), Box([0. 0. 0.], [1. 1. 1.], (3,), float32), Box([0. 0. 0.], [1. 1. 1.], (3,), float32)))), '__env__': (Dict(action_mask:Box([0. 0. 0. 0.], [1. 1. 1. 1.], (4,), float32), avail_actions:Box([0. 0. 0. 0.], [1. 1. 1. 1.], (4,), float32), state:Box([10 10], [10 10], (2,), int32)), Tuple(Discrete(4), Tuple(Box([0. 0. 0.], [1. 1. 1.], (3,), float32), Box([0. 0. 0.], [1. 1. 1.], (3,), float32), Box([0. 0. 0.], [1. 1. 1.], (3,), float32), Box([0. 0. 0.], [1. 1. 1.], (3,), float32))))}\n",
      "2022-03-03 01:58:57,773\tDEBUG rollout_worker.py:1534 -- Creating policy for default_policy\n",
      "2022-03-03 01:58:57,782\tINFO catalog.py:410 -- Wrapping <class 'nclustRL.models.model.RlModel'> as None\n",
      "2022-03-03 01:58:57,786\tINFO torch_policy.py:186 -- TorchPolicy (worker=local) running on 0.0001 GPU(s).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=196113)\u001b[0m 2022-03-03 01:58:57,739\tDEBUG rollout_worker.py:728 -- Created rollout worker with env <ray.rllib.env.base_env._VectorEnvToBaseEnv object at 0x7f0c6fc26fd0> (<TransformObservation<OrderEnforcing<BiclusterEnv<BiclusterEnv-v0>>>>), policies {}\n",
      "/home/pedro.cotovio/Repos/nclustRL/nclustRL/models/model.py:147: UserWarning: Forward is running with random obs, this should only be acceptable during policy inicialization.\n",
      "  warnings.warn('Forward is running with random obs, this should only be acceptable during policy inicialization.')\n",
      "2022-03-03 01:59:25,693\tINFO rollout_worker.py:1555 -- Built policy map: {}\n",
      "2022-03-03 01:59:25,702\tINFO rollout_worker.py:1556 -- Built preprocessor map: {'default_policy': None}\n",
      "2022-03-03 01:59:25,703\tINFO rollout_worker.py:618 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f1802d86d90>}\n",
      "2022-03-03 01:59:25,703\tDEBUG rollout_worker.py:728 -- Created rollout worker with env None (None), policies {}\n",
      "2022-03-03 01:59:25,767\tINFO trainable.py:124 -- Trainable.setup took 48.959 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-03-03 01:59:25,797\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "config = DEFAULT_CONFIG.copy()\n",
    "config['env_config'] = biclustering.binary.basic_v1\n",
    "\n",
    "trainer = Trainer(\n",
    "    trainer=PPOTrainer,\n",
    "    env='BiclusterEnv-v0',\n",
    "    save_dir='/home/pedro.cotovio/Repos/nclustRL/Exp/test_framework2',\n",
    "    name='basic_config',\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's benchmark again the random agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 of 10 done.\n",
      "Episode 2 of 10 done.\n",
      "Episode 3 of 10 done.\n",
      "Episode 4 of 10 done.\n",
      "Episode 5 of 10 done.\n",
      "Episode 6 of 10 done.\n",
      "Episode 7 of 10 done.\n",
      "Episode 8 of 10 done.\n",
      "Episode 9 of 10 done.\n",
      "Episode 10 of 10 done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-1.9714285714285724, 0.04285714285714284)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-03 01:06:28 (running for 09:17:21.61)<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/1 GPUs, 0.0/7.79 GiB heap, 0.0/3.89 GiB objects (0.0/1.0 accelerator_type:T4)<br>Current best trial: 4ac0f_00000 with episode_reward_mean=-1.674791269188329 and parameters={'num_workers': 3, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 341, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 1024, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': True, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': 'general_model_torch', 'custom_model_config': {'fcnet_feats': [256, 256]}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'BiclusterEnv-v0', 'observation_space': None, 'action_space': None, 'env_config': {'shape': [[10, 10], [10, 10]], 'n': 1, 'clusters': [1, 1], 'dataset_settings': {'dstype': {'value': 'Symbolic'}, 'patterns': {'value': [['CONSTANT', 'CONSTANT']]}, 'symbols': {'value': [-1, 1]}, 'bktype': {'value': 'UNIFORM'}, 'clusterdistribution': {'value': [['UNIFORM', 4, 6], ['UNIFORM', 2, 4]]}, 'contiguity': {'value': None}, 'plaidcoherency': {'value': 'NO_OVERLAPPING'}}, 'max_steps': 1000, 'seed': 537}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': False, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': None, 'log_level': 'DEBUG', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'torch', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'explore': False}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': 537, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0.0001, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.3333, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.agents.ppo.ppo_torch_policy.PPOTorchPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /home/pedro.cotovio/Repos/nclustRL/Exp/test_framework2/basic_config/sample_0/PPO_2022-03-02_15-49-06<br>Number of trials: 1/1 (1 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-03 01:06:28,921\tINFO tune.py:626 -- Total run time: 33442.49 seconds (33441.51 seconds for the tuning loop).\n",
      "Sample 1: : 1sample [9:17:23, 33443.01s/sample, metric=-1.77]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'config': {'num_workers': 3,\n",
       "  'create_env_on_driver': False,\n",
       "  'num_envs_per_worker': 1,\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'gamma': 0.99,\n",
       "  'use_critic': True,\n",
       "  'use_gae': True,\n",
       "  'lambda': 1.0,\n",
       "  'kl_coeff': 0.2,\n",
       "  'rollout_fragment_length': 341,\n",
       "  'train_batch_size': 1024,\n",
       "  'sgd_minibatch_size': 128,\n",
       "  'shuffle_sequences': True,\n",
       "  'num_sgd_iter': 30,\n",
       "  'lr': 5e-05,\n",
       "  'lr_schedule': None,\n",
       "  'vf_loss_coeff': 1.0,\n",
       "  'entropy_coeff': 0.0,\n",
       "  'entropy_coeff_schedule': None,\n",
       "  'clip_param': 0.3,\n",
       "  'vf_clip_param': 10.0,\n",
       "  'grad_clip': None,\n",
       "  'kl_target': 0.01,\n",
       "  'optimizer': {},\n",
       "  'horizon': None,\n",
       "  'soft_horizon': False,\n",
       "  'no_done_at_end': False,\n",
       "  'env': 'BiclusterEnv-v0',\n",
       "  'observation_space': None,\n",
       "  'action_space': None,\n",
       "  'env_config': {'shape': [[10, 10], [10, 10]],\n",
       "   'n': 1,\n",
       "   'clusters': [1, 1],\n",
       "   'dataset_settings': {'dstype': {'value': 'Symbolic'},\n",
       "    'patterns': {'value': [['CONSTANT', 'CONSTANT']]},\n",
       "    'symbols': {'value': [-1, 1]},\n",
       "    'bktype': {'value': 'UNIFORM'},\n",
       "    'clusterdistribution': {'value': [['UNIFORM', 4, 6], ['UNIFORM', 2, 4]]},\n",
       "    'contiguity': {'value': None},\n",
       "    'plaidcoherency': {'value': 'NO_OVERLAPPING'}},\n",
       "   'max_steps': 1000,\n",
       "   'seed': 537},\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'env_task_fn': None,\n",
       "  'render_env': False,\n",
       "  'record_env': False,\n",
       "  'clip_rewards': False,\n",
       "  'normalize_actions': True,\n",
       "  'preprocessor_pref': None,\n",
       "  'log_level': 'DEBUG',\n",
       "  'ignore_worker_failures': False,\n",
       "  'log_sys_usage': True,\n",
       "  'framework': 'torch',\n",
       "  'eager_tracing': False,\n",
       "  'explore': True,\n",
       "  'exploration_config': {'type': 'StochasticSampling'},\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_num_episodes': 10,\n",
       "  'evaluation_parallel_to_training': False,\n",
       "  'in_evaluation': False,\n",
       "  'evaluation_config': {'explore': False},\n",
       "  'evaluation_num_workers': 0,\n",
       "  'custom_eval_function': None,\n",
       "  'sample_async': False,\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'synchronize_filters': True,\n",
       "  'compress_observations': False,\n",
       "  'collect_metrics_timeout': 180,\n",
       "  'metrics_smoothing_episodes': 100,\n",
       "  'min_iter_time_s': 0,\n",
       "  'timesteps_per_iteration': 0,\n",
       "  'seed': 537,\n",
       "  'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'num_gpus': 0.0001,\n",
       "  '_fake_gpus': False,\n",
       "  'num_cpus_per_worker': 1,\n",
       "  'num_gpus_per_worker': 0.3333,\n",
       "  'custom_resources_per_worker': {},\n",
       "  'num_cpus_for_driver': 1,\n",
       "  'placement_strategy': 'PACK',\n",
       "  'logger_config': None,\n",
       "  '_disable_preprocessor_api': True,\n",
       "  'model': {'custom_model': 'general_model_torch',\n",
       "   'custom_model_config': {'fcnet_feats': [256, 256]},\n",
       "   'custom_action_dist': None}},\n",
       " 'path': '/home/pedro.cotovio/Repos/nclustRL/Exp/test_framework2/basic_config/sample_0/PPO_2022-03-02_15-49-06/PPO_BiclusterEnv-v0_4ac0f_00000_0_2022-03-02_15-49-06/checkpoint_000010/checkpoint-10',\n",
       " 'metric': -1.7674513395101639,\n",
       " 'df':    episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
       " 0                -1.5           -1.857143            -1.674791   \n",
       " \n",
       "    episode_len_mean  episodes_this_iter  num_healthy_workers  timesteps_total  \\\n",
       " 0            1001.0                   3                    3          2046000   \n",
       " \n",
       "    timesteps_this_iter  agent_timesteps_total  done  ...  \\\n",
       " 0                    0                2046000  True  ...   \n",
       " \n",
       "    config/shuffle_sequences  config/soft_horizon config/synchronize_filters  \\\n",
       " 0                      True                False                       True   \n",
       " \n",
       "   config/timesteps_per_iteration config/train_batch_size  config/use_critic  \\\n",
       " 0                              0                    1024               True   \n",
       " \n",
       "    config/use_gae  config/vf_clip_param  config/vf_loss_coeff  \\\n",
       " 0            True                  10.0                   1.0   \n",
       " \n",
       "                                               logdir  \n",
       " 0  /home/pedro.cotovio/Repos/nclustRL/Exp/test_fr...  \n",
       " \n",
       " [1 rows x 123 columns]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_checkpoint = trainer.train(stop_iters=1000)\n",
    "best_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-03 01:59:27,466\tINFO trainable.py:467 -- Restored on 10.211.200.145 from checkpoint: /home/pedro.cotovio/Repos/nclustRL/Exp/test_framework2/basic_config/sample_0/PPO_2022-03-02_15-49-06/PPO_BiclusterEnv-v0_4ac0f_00000_0_2022-03-02_15-49-06/checkpoint_001000/checkpoint-1000\n",
      "2022-03-03 01:59:27,467\tINFO trainable.py:475 -- Current state after restoring: {'_iteration': 1000, '_timesteps_total': 0, '_time_total': 33344.96429896355, '_episodes_total': 2043}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 of 10 done.\n",
      "Episode 2 of 10 done.\n",
      "Episode 3 of 10 done.\n",
      "Episode 4 of 10 done.\n",
      "Episode 5 of 10 done.\n",
      "Episode 6 of 10 done.\n",
      "Episode 7 of 10 done.\n",
      "Episode 8 of 10 done.\n",
      "Episode 9 of 10 done.\n",
      "Episode 10 of 10 done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-1.8588095238095246, 0.1411904761904762)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.load('/home/pedro.cotovio/Repos/nclustRL/Exp/test_framework2/basic_config/sample_0/PPO_2022-03-02_15-49-06/PPO_BiclusterEnv-v0_4ac0f_00000_0_2022-03-02_15-49-06/checkpoint_001000/checkpoint-1000')\n",
    "trainer.test(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Train basic v2\n",
    " - Hyperparam tune (50 it)\n",
    "- train again with new params\n",
    "    - test trained and random (tuned)\n",
    "    - render actions random and trained (tuned)\n",
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-03 02:06:39,800\tWARNING ppo.py:143 -- `train_batch_size` (1024) cannot be achieved with your other settings (num_workers=3 num_envs_per_worker=1 rollout_fragment_length=341)! Auto-adjusting `rollout_fragment_length` to 341.\n",
      "2022-03-03 02:06:39,801\tINFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=197507)\u001b[0m Using backend: pytorch\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=197504)\u001b[0m Using backend: pytorch\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=197506)\u001b[0m Using backend: pytorch\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=197507)\u001b[0m 2022-03-03 02:06:45,742\tINFO worker.py:852 -- Calling ray.init() again after it has already been called.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=197504)\u001b[0m 2022-03-03 02:06:45,757\tINFO worker.py:852 -- Calling ray.init() again after it has already been called.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=197506)\u001b[0m 2022-03-03 02:06:45,987\tINFO worker.py:852 -- Calling ray.init() again after it has already been called.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=197507)\u001b[0m 2022-03-03 02:06:57,337\tINFO rollout_worker.py:1705 -- Validating sub-env at vector index=0 ... (ok)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=197507)\u001b[0m 2022-03-03 02:06:57,338\tDEBUG rollout_worker.py:1534 -- Creating policy for default_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=197504)\u001b[0m 2022-03-03 02:06:57,337\tINFO rollout_worker.py:1705 -- Validating sub-env at vector index=0 ... (ok)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=197504)\u001b[0m 2022-03-03 02:06:57,337\tDEBUG rollout_worker.py:1534 -- Creating policy for default_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=197506)\u001b[0m 2022-03-03 02:06:57,337\tINFO rollout_worker.py:1705 -- Validating sub-env at vector index=0 ... (ok)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=197506)\u001b[0m 2022-03-03 02:06:57,338\tDEBUG rollout_worker.py:1534 -- Creating policy for default_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=197504)\u001b[0m 2022-03-03 02:06:57,367\tINFO catalog.py:410 -- Wrapping <class 'nclustRL.models.model.RlModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=197506)\u001b[0m 2022-03-03 02:06:57,372\tINFO catalog.py:410 -- Wrapping <class 'nclustRL.models.model.RlModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=197507)\u001b[0m 2022-03-03 02:06:57,506\tINFO catalog.py:410 -- Wrapping <class 'nclustRL.models.model.RlModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=197507)\u001b[0m 2022-03-03 02:06:57,609\tINFO torch_policy.py:186 -- TorchPolicy (worker=1) running on 0.3333 GPU(s).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=197507)\u001b[0m /home/pedro.cotovio/Repos/nclustRL/nclustRL/models/model.py:147: UserWarning: Forward is running with random obs, this should only be acceptable during policy inicialization.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=197507)\u001b[0m   warnings.warn('Forward is running with random obs, this should only be acceptable during policy inicialization.')\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=197504)\u001b[0m 2022-03-03 02:06:57,611\tINFO torch_policy.py:186 -- TorchPolicy (worker=3) running on 0.3333 GPU(s).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=197504)\u001b[0m /home/pedro.cotovio/Repos/nclustRL/nclustRL/models/model.py:147: UserWarning: Forward is running with random obs, this should only be acceptable during policy inicialization.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=197504)\u001b[0m   warnings.warn('Forward is running with random obs, this should only be acceptable during policy inicialization.')\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=197506)\u001b[0m 2022-03-03 02:06:57,611\tINFO torch_policy.py:186 -- TorchPolicy (worker=2) running on 0.3333 GPU(s).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=197506)\u001b[0m /home/pedro.cotovio/Repos/nclustRL/nclustRL/models/model.py:147: UserWarning: Forward is running with random obs, this should only be acceptable during policy inicialization.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=197506)\u001b[0m   warnings.warn('Forward is running with random obs, this should only be acceptable during policy inicialization.')\n",
      "2022-03-03 02:07:03,002\tINFO worker_set.py:104 -- Inferred observation/action spaces from remote worker (local worker has no env): {'default_policy': (Dict(action_mask:Box([0. 0. 0. 0.], [1. 1. 1. 1.], (4,), float32), avail_actions:Box([0. 0. 0. 0.], [1. 1. 1. 1.], (4,), float32), state:Box([10 10], [10 10], (2,), int32)), Tuple(Discrete(4), Tuple(Box([0. 0. 0.], [1. 1. 1.], (3,), float32), Box([0. 0. 0.], [1. 1. 1.], (3,), float32), Box([0. 0. 0.], [1. 1. 1.], (3,), float32), Box([0. 0. 0.], [1. 1. 1.], (3,), float32)))), '__env__': (Dict(action_mask:Box([0. 0. 0. 0.], [1. 1. 1. 1.], (4,), float32), avail_actions:Box([0. 0. 0. 0.], [1. 1. 1. 1.], (4,), float32), state:Box([10 10], [10 10], (2,), int32)), Tuple(Discrete(4), Tuple(Box([0. 0. 0.], [1. 1. 1.], (3,), float32), Box([0. 0. 0.], [1. 1. 1.], (3,), float32), Box([0. 0. 0.], [1. 1. 1.], (3,), float32), Box([0. 0. 0.], [1. 1. 1.], (3,), float32))))}\n",
      "2022-03-03 02:07:03,004\tDEBUG rollout_worker.py:1534 -- Creating policy for default_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=197507)\u001b[0m 2022-03-03 02:07:02,983\tDEBUG rollout_worker.py:728 -- Created rollout worker with env <ray.rllib.env.base_env._VectorEnvToBaseEnv object at 0x7fbd484dd430> (<TransformObservation<OrderEnforcing<BiclusterEnv<BiclusterEnv-v0>>>>), policies {}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=197504)\u001b[0m 2022-03-03 02:07:02,914\tDEBUG rollout_worker.py:728 -- Created rollout worker with env <ray.rllib.env.base_env._VectorEnvToBaseEnv object at 0x7fe8839aa4f0> (<TransformObservation<OrderEnforcing<BiclusterEnv<BiclusterEnv-v0>>>>), policies {}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=197506)\u001b[0m 2022-03-03 02:07:02,916\tDEBUG rollout_worker.py:728 -- Created rollout worker with env <ray.rllib.env.base_env._VectorEnvToBaseEnv object at 0x7f45b77e4250> (<TransformObservation<OrderEnforcing<BiclusterEnv<BiclusterEnv-v0>>>>), policies {}\n",
      "2022-03-03 02:07:03,017\tINFO catalog.py:410 -- Wrapping <class 'nclustRL.models.model.RlModel'> as None\n",
      "2022-03-03 02:07:03,024\tINFO torch_policy.py:186 -- TorchPolicy (worker=local) running on 0.0001 GPU(s).\n",
      "/home/pedro.cotovio/Repos/nclustRL/nclustRL/models/model.py:147: UserWarning: Forward is running with random obs, this should only be acceptable during policy inicialization.\n",
      "  warnings.warn('Forward is running with random obs, this should only be acceptable during policy inicialization.')\n",
      "2022-03-03 02:07:30,501\tINFO rollout_worker.py:1555 -- Built policy map: {}\n",
      "2022-03-03 02:07:30,511\tINFO rollout_worker.py:1556 -- Built preprocessor map: {'default_policy': None}\n",
      "2022-03-03 02:07:30,511\tINFO rollout_worker.py:618 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f9b6750ea90>}\n",
      "2022-03-03 02:07:30,512\tDEBUG rollout_worker.py:728 -- Created rollout worker with env None (None), policies {}\n",
      "2022-03-03 02:07:30,638\tINFO trainable.py:124 -- Trainable.setup took 50.839 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-03-03 02:07:30,661\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
      "2022-03-03 02:07:30,857\tINFO trainable.py:467 -- Restored on 10.211.200.145 from checkpoint: /home/pedro.cotovio/Repos/nclustRL/Exp/test_framework2/basic_config/sample_0/PPO_2022-03-02_15-49-06/PPO_BiclusterEnv-v0_4ac0f_00000_0_2022-03-02_15-49-06/checkpoint_001000/checkpoint-1000\n",
      "2022-03-03 02:07:30,857\tINFO trainable.py:475 -- Current state after restoring: {'_iteration': 1000, '_timesteps_total': 0, '_time_total': 33344.96429896355, '_episodes_total': 2043}\n"
     ]
    }
   ],
   "source": [
    "config = DEFAULT_CONFIG.copy()\n",
    "config['env_config'] = biclustering.binary.basic_v2\n",
    "\n",
    "trainer = Trainer(\n",
    "    trainer=PPOTrainer,\n",
    "    env='BiclusterEnv-v0',\n",
    "    save_dir='/home/pedro.cotovio/Repos/nclustRL/Exp/test_framework2',\n",
    "    name='basic_config_hypertune',\n",
    "    config=config\n",
    ")\n",
    "\n",
    "trainer.load(checkpoint='/home/pedro.cotovio/Repos/nclustRL/Exp/test_framework2/basic_config/sample_0/PPO_2022-03-02_15-49-06/PPO_BiclusterEnv-v0_4ac0f_00000_0_2022-03-02_15-49-06/checkpoint_001000/checkpoint-1000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-03 12:06:04 (running for 09:56:29.98)<br>Memory usage on this node: 14.9/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>PopulationBasedTraining: 30 checkpoints, 22 perturbs<br>Resources requested: 4.0/4 CPUs, 1.0/1 GPUs, 0.0/7.77 GiB heap, 0.0/3.88 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /home/pedro.cotovio/Repos/nclustRL/Exp/test_framework2/basic_config_hypertune/sample_0/PPO_2022-03-03_02-09-34<br>Number of trials: 8/8 (7 ERROR, 1 RUNNING)<br>Number of errored trials: 7<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                    </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BiclusterEnv-v0_f83fe_00000</td><td style=\"text-align: right;\">           1</td><td>/home/pedro.cotovio/Repos/nclustRL/Exp/test_framework2/basic_config_hypertune/sample_0/PPO_2022-03-03_02-09-34/PPO_BiclusterEnv-v0_f83fe_00000_0_2022-03-03_02-09-34/error.txt</td></tr>\n",
       "<tr><td>PPO_BiclusterEnv-v0_f83fe_00001</td><td style=\"text-align: right;\">           1</td><td>/home/pedro.cotovio/Repos/nclustRL/Exp/test_framework2/basic_config_hypertune/sample_0/PPO_2022-03-03_02-09-34/PPO_BiclusterEnv-v0_f83fe_00001_1_2022-03-03_02-09-34/error.txt</td></tr>\n",
       "<tr><td>PPO_BiclusterEnv-v0_f83fe_00002</td><td style=\"text-align: right;\">           1</td><td>/home/pedro.cotovio/Repos/nclustRL/Exp/test_framework2/basic_config_hypertune/sample_0/PPO_2022-03-03_02-09-34/PPO_BiclusterEnv-v0_f83fe_00002_2_2022-03-03_02-09-47/error.txt</td></tr>\n",
       "<tr><td>PPO_BiclusterEnv-v0_f83fe_00004</td><td style=\"text-align: right;\">           1</td><td>/home/pedro.cotovio/Repos/nclustRL/Exp/test_framework2/basic_config_hypertune/sample_0/PPO_2022-03-03_02-09-34/PPO_BiclusterEnv-v0_f83fe_00004_4_2022-03-03_02-14-31/error.txt</td></tr>\n",
       "<tr><td>PPO_BiclusterEnv-v0_f83fe_00005</td><td style=\"text-align: right;\">           1</td><td>/home/pedro.cotovio/Repos/nclustRL/Exp/test_framework2/basic_config_hypertune/sample_0/PPO_2022-03-03_02-09-34/PPO_BiclusterEnv-v0_f83fe_00005_5_2022-03-03_02-16-33/error.txt</td></tr>\n",
       "<tr><td>PPO_BiclusterEnv-v0_f83fe_00006</td><td style=\"text-align: right;\">           1</td><td>/home/pedro.cotovio/Repos/nclustRL/Exp/test_framework2/basic_config_hypertune/sample_0/PPO_2022-03-03_02-09-34/PPO_BiclusterEnv-v0_f83fe_00006_6_2022-03-03_02-18-49/error.txt</td></tr>\n",
       "<tr><td>PPO_BiclusterEnv-v0_f83fe_00007</td><td style=\"text-align: right;\">           1</td><td>/home/pedro.cotovio/Repos/nclustRL/Exp/test_framework2/basic_config_hypertune/sample_0/PPO_2022-03-03_02-09-34/PPO_BiclusterEnv-v0_f83fe_00007_7_2022-03-03_02-21-09/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-03 12:06:04,713\tERROR tune.py:622 -- Trials did not complete: [PPO_BiclusterEnv-v0_f83fe_00000, PPO_BiclusterEnv-v0_f83fe_00001, PPO_BiclusterEnv-v0_f83fe_00002, PPO_BiclusterEnv-v0_f83fe_00003, PPO_BiclusterEnv-v0_f83fe_00004, PPO_BiclusterEnv-v0_f83fe_00005, PPO_BiclusterEnv-v0_f83fe_00006, PPO_BiclusterEnv-v0_f83fe_00007]\n",
      "2022-03-03 12:06:04,715\tINFO tune.py:626 -- Total run time: 35790.50 seconds (35789.93 seconds for the tuning loop).\n",
      "2022-03-03 12:06:04,715\tWARNING tune.py:630 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n",
      "Sample 1: : 0sample [9:56:32, ?sample/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No `metric` has been passed and  `default_metric` has not been set. Please specify the `metric` parameter.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_197366/2166116366.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m best_checkpoint = trainer.train(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGRID_PPO_PBT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/home/pedro.cotovio/Repos/nclustRL/Exp/test_framework2/basic_config/sample_0/PPO_2022-03-02_15-49-06/PPO_BiclusterEnv-v0_4ac0f_00000_0_2022-03-02_15-49-06/checkpoint_001000/checkpoint-1000'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Repos/nclustRL/nclustRL/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_samples, metric, mode, checkpoint_freq, stop, stop_iters, stop_metric, checkpoint, resume, verbose, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                 checkpoints = analysis.get_trial_checkpoints_paths(\n\u001b[0;32m--> 169\u001b[0;31m                     trial=analysis.get_best_trial(\n\u001b[0m\u001b[1;32m    170\u001b[0m                         \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                         mode=mode), metric=metric)\n",
      "\u001b[0;32m~/Repos/nclustRL/venv/lib/python3.8/site-packages/ray/tune/analysis/experiment_analysis.py\u001b[0m in \u001b[0;36mget_best_trial\u001b[0;34m(self, metric, mode, scope, filter_nan_and_inf)\u001b[0m\n\u001b[1;32m    470\u001b[0m                 \u001b[0mthe\u001b[0m \u001b[0mbest\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \"\"\"\n\u001b[0;32m--> 472\u001b[0;31m         \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Repos/nclustRL/venv/lib/python3.8/site-packages/ray/tune/analysis/experiment_analysis.py\u001b[0m in \u001b[0;36m_validate_metric\u001b[0;34m(self, metric)\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_metric\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    705\u001b[0m                 \u001b[0;34m\"No `metric` has been passed and  `default_metric` has \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \"not been set. Please specify the `metric` parameter.\")\n",
      "\u001b[0;31mValueError\u001b[0m: No `metric` has been passed and  `default_metric` has not been set. Please specify the `metric` parameter."
     ]
    }
   ],
   "source": [
    "best_checkpoint = trainer.train(\n",
    "    stop=False, \n",
    "    num_samples=8, \n",
    "    scheduler=PPO_PBT,\n",
    "    checkpoint='/home/pedro.cotovio/Repos/nclustRL/Exp/test_framework2/basic_config/sample_0/PPO_2022-03-02_15-49-06/PPO_BiclusterEnv-v0_4ac0f_00000_0_2022-03-02_15-49-06/checkpoint_001000/checkpoint-1000',\n",
    "    metric=None,\n",
    "    mode=None\n",
    ")\n",
    "best_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This trial cannot run locally do to memory and gpu contrains, it will be run externally in a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-06 14:12:01,624\tWARNING ppo.py:143 -- `train_batch_size` (1024) cannot be achieved with your other settings (num_workers=3 num_envs_per_worker=1 rollout_fragment_length=341)! Auto-adjusting `rollout_fragment_length` to 341.\n",
      "2022-03-06 14:12:01,624\tINFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59108)\u001b[0m Using backend: pytorch\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59110)\u001b[0m Using backend: pytorch\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59109)\u001b[0m Using backend: pytorch\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59110)\u001b[0m 2022-03-06 14:12:07,222\tINFO worker.py:852 -- Calling ray.init() again after it has already been called.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59108)\u001b[0m 2022-03-06 14:12:07,642\tINFO worker.py:852 -- Calling ray.init() again after it has already been called.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59109)\u001b[0m 2022-03-06 14:12:07,567\tINFO worker.py:852 -- Calling ray.init() again after it has already been called.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59108)\u001b[0m 2022-03-06 14:12:18,261\tINFO rollout_worker.py:1705 -- Validating sub-env at vector index=0 ... (ok)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59108)\u001b[0m 2022-03-06 14:12:18,261\tDEBUG rollout_worker.py:1534 -- Creating policy for default_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59110)\u001b[0m 2022-03-06 14:12:18,260\tINFO rollout_worker.py:1705 -- Validating sub-env at vector index=0 ... (ok)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59110)\u001b[0m 2022-03-06 14:12:18,261\tDEBUG rollout_worker.py:1534 -- Creating policy for default_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59109)\u001b[0m 2022-03-06 14:12:18,260\tINFO rollout_worker.py:1705 -- Validating sub-env at vector index=0 ... (ok)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59109)\u001b[0m 2022-03-06 14:12:18,261\tDEBUG rollout_worker.py:1534 -- Creating policy for default_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59108)\u001b[0m 2022-03-06 14:12:18,404\tINFO catalog.py:410 -- Wrapping <class 'nclustRL.models.model.RlModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59110)\u001b[0m 2022-03-06 14:12:18,410\tINFO catalog.py:410 -- Wrapping <class 'nclustRL.models.model.RlModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59109)\u001b[0m 2022-03-06 14:12:18,402\tINFO catalog.py:410 -- Wrapping <class 'nclustRL.models.model.RlModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59108)\u001b[0m 2022-03-06 14:12:18,540\tINFO torch_policy.py:186 -- TorchPolicy (worker=2) running on 0.3333 GPU(s).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59108)\u001b[0m /home/pedro.cotovio/Repos/nclustRL/nclustRL/models/model.py:147: UserWarning: Forward is running with random obs, this should only be acceptable during policy inicialization.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59108)\u001b[0m   warnings.warn('Forward is running with random obs, this should only be acceptable during policy inicialization.')\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59110)\u001b[0m 2022-03-06 14:12:18,543\tINFO torch_policy.py:186 -- TorchPolicy (worker=3) running on 0.3333 GPU(s).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59110)\u001b[0m /home/pedro.cotovio/Repos/nclustRL/nclustRL/models/model.py:147: UserWarning: Forward is running with random obs, this should only be acceptable during policy inicialization.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59110)\u001b[0m   warnings.warn('Forward is running with random obs, this should only be acceptable during policy inicialization.')\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59109)\u001b[0m 2022-03-06 14:12:18,544\tINFO torch_policy.py:186 -- TorchPolicy (worker=1) running on 0.3333 GPU(s).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59109)\u001b[0m /home/pedro.cotovio/Repos/nclustRL/nclustRL/models/model.py:147: UserWarning: Forward is running with random obs, this should only be acceptable during policy inicialization.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59109)\u001b[0m   warnings.warn('Forward is running with random obs, this should only be acceptable during policy inicialization.')\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59110)\u001b[0m 2022-03-06 14:12:24,015\tDEBUG rollout_worker.py:728 -- Created rollout worker with env <ray.rllib.env.base_env._VectorEnvToBaseEnv object at 0x7fc3202ef610> (<TransformObservation<OrderEnforcing<BiclusterEnv<BiclusterEnv-v0>>>>), policies {}\n",
      "2022-03-06 14:12:24,084\tINFO worker_set.py:104 -- Inferred observation/action spaces from remote worker (local worker has no env): {'default_policy': (Dict(action_mask:Box([0. 0. 0. 0.], [1. 1. 1. 1.], (4,), float32), avail_actions:Box([0. 0. 0. 0.], [1. 1. 1. 1.], (4,), float32), state:Box([6 6], [6 6], (2,), int32)), Tuple(Discrete(4), Tuple(Box([0. 0. 0.], [1. 1. 1.], (3,), float32), Box([0. 0. 0.], [1. 1. 1.], (3,), float32), Box([0. 0. 0.], [1. 1. 1.], (3,), float32), Box([0. 0. 0.], [1. 1. 1.], (3,), float32)))), '__env__': (Dict(action_mask:Box([0. 0. 0. 0.], [1. 1. 1. 1.], (4,), float32), avail_actions:Box([0. 0. 0. 0.], [1. 1. 1. 1.], (4,), float32), state:Box([6 6], [6 6], (2,), int32)), Tuple(Discrete(4), Tuple(Box([0. 0. 0.], [1. 1. 1.], (3,), float32), Box([0. 0. 0.], [1. 1. 1.], (3,), float32), Box([0. 0. 0.], [1. 1. 1.], (3,), float32), Box([0. 0. 0.], [1. 1. 1.], (3,), float32))))}\n",
      "2022-03-06 14:12:24,086\tDEBUG rollout_worker.py:1534 -- Creating policy for default_policy\n",
      "2022-03-06 14:12:24,101\tINFO catalog.py:410 -- Wrapping <class 'nclustRL.models.model.RlModel'> as None\n",
      "2022-03-06 14:12:24,113\tINFO torch_policy.py:186 -- TorchPolicy (worker=local) running on 0.0001 GPU(s).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59108)\u001b[0m 2022-03-06 14:12:24,148\tDEBUG rollout_worker.py:728 -- Created rollout worker with env <ray.rllib.env.base_env._VectorEnvToBaseEnv object at 0x7f3d67dea100> (<TransformObservation<OrderEnforcing<BiclusterEnv<BiclusterEnv-v0>>>>), policies {}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59109)\u001b[0m 2022-03-06 14:12:24,072\tDEBUG rollout_worker.py:728 -- Created rollout worker with env <ray.rllib.env.base_env._VectorEnvToBaseEnv object at 0x7faf3ede0100> (<TransformObservation<OrderEnforcing<BiclusterEnv<BiclusterEnv-v0>>>>), policies {}\n",
      "/home/pedro.cotovio/Repos/nclustRL/nclustRL/models/model.py:147: UserWarning: Forward is running with random obs, this should only be acceptable during policy inicialization.\n",
      "  warnings.warn('Forward is running with random obs, this should only be acceptable during policy inicialization.')\n",
      "2022-03-06 14:12:49,900\tINFO rollout_worker.py:1555 -- Built policy map: {}\n",
      "2022-03-06 14:12:49,912\tINFO rollout_worker.py:1556 -- Built preprocessor map: {'default_policy': None}\n",
      "2022-03-06 14:12:49,914\tINFO rollout_worker.py:618 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f1b41018cd0>}\n",
      "2022-03-06 14:12:49,915\tDEBUG rollout_worker.py:728 -- Created rollout worker with env None (None), policies {}\n",
      "2022-03-06 14:12:50,030\tINFO trainable.py:124 -- Trainable.setup took 48.408 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-03-06 14:12:50,062\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "config = DEFAULT_CONFIG.copy()\n",
    "config['env_config'] = biclustering.binary.basic_v2\n",
    "\n",
    "trainer = Trainer(\n",
    "    trainer=PPOTrainer,\n",
    "    env='BiclusterEnv-v0',\n",
    "    save_dir='/home/pedro.cotovio/Repos/nclustRL/Exp/test_framework2',\n",
    "    name='basic_v2_config',\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 of 10 done.\n",
      "Episode 2 of 10 done.\n",
      "Episode 3 of 10 done.\n",
      "Episode 4 of 10 done.\n",
      "Episode 5 of 10 done.\n",
      "Episode 6 of 10 done.\n",
      "Episode 7 of 10 done.\n",
      "Episode 8 of 10 done.\n",
      "Episode 9 of 10 done.\n",
      "Episode 10 of 10 done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-1.9142857142857153, 0.08571428571428567)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 18:49:58 (running for 04:31:07.40)<br>Memory usage on this node: 14.2/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/4 CPUs, 1.0/1 GPUs, 0.0/7.92 GiB heap, 0.0/3.96 GiB objects (0.0/1.0 accelerator_type:T4)<br>Current best trial: 58725_00000 with episode_reward_mean=-1.2226448484848493 and parameters={'num_workers': 3, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 341, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 1024, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': True, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': 'general_model_torch', 'custom_model_config': {'fcnet_feats': [256, 256]}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'BiclusterEnv-v0', 'observation_space': None, 'action_space': None, 'env_config': {'shape': [[6, 6], [6, 6]], 'n': 1, 'clusters': [1, 1], 'dataset_settings': {'dstype': {'value': 'Symbolic'}, 'patterns': {'value': [['CONSTANT', 'CONSTANT']]}, 'symbols': {'value': [-1, 1]}, 'bktype': {'value': 'UNIFORM'}, 'clusterdistribution': {'value': [['UNIFORM', 4, 4], ['UNIFORM', 3, 3]]}, 'contiguity': {'value': None}, 'plaidcoherency': {'value': 'NO_OVERLAPPING'}}, 'max_steps': 1000, 'seed': 537}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': False, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': None, 'log_level': 'DEBUG', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'torch', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'explore': False}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': 537, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0.0001, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.3333, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.agents.ppo.ppo_torch_policy.PPOTorchPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /home/pedro.cotovio/Repos/nclustRL/Exp/test_framework2/basic_v2_config/sample_0/PPO_2022-03-06_14-18-50<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_checkpoint = trainer.train(stop_iters=1000)\n",
    "best_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nclustRL.models import RlModel, GraphEncoder\n",
    "from nclustRL.models.model import FcNet\n",
    "from nclustRL.utils.helper import generate_dummy_obs, transform_obs\n",
    "import nclustenv\n",
    "from nclustenv.configs.biclustering.binary import base\n",
    "from nclustRL.configs.default_configs import MODEL_DEFAULTS\n",
    "from gym.wrappers import TransformObservation\n",
    "\n",
    "import torch as th \n",
    "from ray.rllib.utils.torch_utils import FLOAT_MIN, FLOAT_MAX\n",
    "\n",
    "BATCH = 32\n",
    "\n",
    "def make_env():\n",
    "    return TransformObservation(nclustenv.make('BiclusterEnv-v0', **base), transform_obs)\n",
    "\n",
    "def init_rlmodel():\n",
    "\n",
    "    env = make_env()\n",
    "    \n",
    "    model = RlModel(env.observation_space, env.action_space, 28, MODEL_DEFAULTS, '')\n",
    "    return model.cuda()\n",
    "\n",
    "def generate_dummy_dict():\n",
    "    env = make_env()\n",
    "    return {'obs': [env.reset() for _ in range(BATCH)]}\n",
    "\n",
    "rlmodel = init_rlmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro.cotovio/Repos/nclustRL/nclustRL/models/model.py:144: UserWarning: Forward is running with random obs, this should only be acceptable during policy inicialization.\n",
      "  warnings.warn('Forward is running with random obs, this should only be acceptable during policy inicialization.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = make_env()\n",
    "\n",
    "model = GraphEncoder(env.observation_space['state'].n, [64], ['elem'])\n",
    "model.cuda()\n",
    "obs_flat = rlmodel._generate_dummy_obs(th.zeros((BATCH, 2)))\n",
    "hg = model(obs_flat)\n",
    "hg.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = FcNet(64, [256, 256])\n",
    "model2.cuda()\n",
    "logits = model2(hg)\n",
    "logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 24])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_params = rlmodel._param_branch(logits)\n",
    "_params.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_actions = rlmodel._action_branch(logits)\n",
    "_actions.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rlmodel._value_branch(logits).squeeze(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 28])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "\n",
    "avail_actions = th.tensor(obs[\"avail_actions\"], device=obs_flat.device).unsqueeze(0).repeat(32, 1)\n",
    "action_mask = th.tensor(obs[\"action_mask\"], device=obs_flat.device).unsqueeze(0).repeat(32, 1)\n",
    "\n",
    "intent_vector = th.unsqueeze(_actions, 1)\n",
    "action_logits = th.sum(avail_actions * intent_vector, dim=1)\n",
    "inf_mask = th.clamp(th.log(action_mask), FLOAT_MIN, FLOAT_MAX)\n",
    "\n",
    "th.cat(((action_logits + inf_mask), _params), dim=1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'is_block'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_236088/3686454552.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdgl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Repos/nclustRL/venv/lib/python3.8/site-packages/dgl/batch.py\u001b[0m in \u001b[0;36mbatch\u001b[0;34m(graphs, ndata, edata, node_attrs, edge_attrs)\u001b[0m\n\u001b[1;32m    166\u001b[0m         raise DGLError('Invalid argument edata: must be a string list but got {}.'.format(\n\u001b[1;32m    167\u001b[0m             type(edata)))\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_block\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mDGLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Batching a MFG is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Repos/nclustRL/venv/lib/python3.8/site-packages/dgl/batch.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    166\u001b[0m         raise DGLError('Invalid argument edata: must be a string list but got {}.'.format(\n\u001b[1;32m    167\u001b[0m             type(edata)))\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_block\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mDGLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Batching a MFG is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'is_block'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import dgl\n",
    "dgl.batch(list(np.array([env.reset() for _ in range(10)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes={'col': 100, 'row': 1000},\n",
       "      num_edges={('row', 'elem', 'col'): 10000},\n",
       "      metagraph=[('row', 'col', 'elem')])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgl.batch(list(np.array([env.reset()['state'] for _ in range(10)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
